================================================
FILE: README.md
================================================
# üåå **LLMgine**

LLMgine is a _pattern-driven_ framework for building **production-grade, tool-augmented LLM applications** in Python.  
It offers a clean separation between _**engines**_ (conversation logic), _**models/providers**_ (LLM back-ends), _**tools**_ (function calling), a streaming **message-bus** for commands & events, and opt-in **observability**.  
Think _FastAPI_ for web servers or _Celery_ for tasks‚ÄîLLMgine plays the same role for complex, chat-oriented AI.

---

## ‚ú® Feature Highlights
| Area | What you get | Key files |
|------|--------------|-----------|
| **Engines** | Plug-n-play `Engine` subclasses (`SinglePassEngine`, `ToolChatEngine`, ‚Ä¶) with session isolation, tool-loop orchestration, and CLI front-ends | `engines/*.py`, `src/llmgine/llm/engine/` |
| **Message Bus** | Async **command bus** (1 handler) + **event bus** (N listeners) + **sessions** for scoped handlers | `src/llmgine/bus/` |
| **Tooling** | Declarative function-to-tool registration, multi-provider JSON-schema parsing (OpenAI, Claude, DeepSeek), async execution pipeline | `src/llmgine/llm/tools/` |
| **Providers / Models** | Wrapper classes for OpenAI, OpenRouter, Gemini 2.5 Flash etc. _without locking you in_ | `src/llmgine/llm/providers/`, `src/llmgine/llm/models/` |
| **Context Management** | Simple and in-memory chat history managers, event-emitting for retrieval/update | `src/llmgine/llm/context/` |
| **UI** | Rich-powered interactive CLI (`EngineCLI`) with live spinners, confirmation prompts, tool result panes | `src/llmgine/ui/cli/` |
| **Observability** | Console + JSONL file handlers, per-event metadata, easy custom sinks | `src/llmgine/observability/` |
| **Bootstrap** | One-liner `ApplicationBootstrap` that wires logging, bus startup, and observability | `src/llmgine/bootstrap.py` |

---

## üèóÔ∏è High-Level Architecture

```mermaid
flowchart TD
    %% Nodes
    AppBootstrap["ApplicationBootstrap"]
    Bus["MessageBus<br/>(async loop)"]
    Obs["Observability<br/>Handlers"]
    Eng["Engine(s)"]
    TM["ToolManager"]
    Tools["Your&nbsp;Tools"]
    Session["BusSession"]
    CLI["CLI / UI"]

    %% Edges
    AppBootstrap -->|starts| Bus

    Bus -->|events| Obs
    Bus -->|commands| Eng
    Bus -->|events| Session

    Eng -- status --> Bus
    Eng -- tool_calls --> TM

    TM -- executes --> Tools
    Tools -- ToolResult --> CLI

    Session --> CLI
```

*Every component communicates _only_ through the bus, so engines, tools, and UIs remain fully decoupled.*

---

## üöÄ Quick Start

### 1. Install

```bash
git clone https://github.com/your-org/llmgine.git
cd llmgine
python -m venv .venv && source .venv/bin/activate
pip install -e ".[openai]"   # extras: openai, openrouter, dev, ‚Ä¶
export OPENAI_API_KEY="sk-‚Ä¶" # or OPENROUTER_API_KEY / GEMINI_API_KEY
```

### 2. Run the demo CLI

```bash
python -m llmgine.engines.single_pass_engine  # pirate translator
# or
python -m llmgine.engines.tool_chat_engine    # automatic tool loop
```

You‚Äôll get an interactive prompt with live status updates and tool execution logs.

---

## üßë‚Äçüíª Building Your Own Engine

```python
from llmgine.llm.engine.engine import Engine
from llmgine.messages.commands import Command, CommandResult
from llmgine.bus.bus import MessageBus

class MyCommand(Command):
    prompt: str = ""

class MyEngine(Engine):
    def __init__(self, session_id: str):
        self.session_id = session_id
        self.bus = MessageBus()

    async def handle_command(self, cmd: MyCommand) -> CommandResult:
        await self.bus.publish(Status("thinking", session_id=self.session_id))
        # call LLM or custom logic here ‚Ä¶
        answer = f"Echo: {cmd.prompt}"
        await self.bus.publish(Status("finished", session_id=self.session_id))
        return CommandResult(success=True, result=answer)

# Wire into CLI
from llmgine.ui.cli.cli import EngineCLI
chat = EngineCLI(session_id="demo")
chat.register_engine(MyEngine("demo"))
chat.register_engine_command(MyCommand, MyEngine("demo").handle_command)
await chat.main()
```

---

## üîß Registering Tools in 3 Lines

```python
from llmgine.llm.tools.tool import Parameter
from llmgine.engines.tool_chat_engine import ToolChatEngine

def get_weather(city: str):
    """Return current temperature for a city.
    Args:
        city: Name of the city
    """
    return f"{city}: 17 ¬∞C"

engine = ToolChatEngine(session_id="demo")
await engine.register_tool(get_weather)               # ‚Üê introspection magic ‚ú®
```

The engine now follows the **OpenAI function-calling loop**:

```
User ‚Üí Engine ‚Üí LLM (asks to call get_weather) ‚Üí ToolManager ‚Üí get_weather()
          ‚Üë                                        ‚Üì
          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    context update   ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (loops until no tool calls)
```

---

## üì∞ Message Bus in Depth

```python
from llmgine.bus.bus import MessageBus
from llmgine.bus.session import BusSession

bus = MessageBus()
await bus.start()

class Ping(Command): pass
class Pong(Event): msg: str = "pong!"

async def ping_handler(cmd: Ping):
    await bus.publish(Pong(session_id=cmd.session_id))
    return CommandResult(success=True)

with bus.create_session() as sess:
    sess.register_command_handler(Ping, ping_handler)
    sess.register_event_handler(Pong, lambda e: print(e.msg))
    await sess.execute_with_session(Ping())      # prints ‚Äúpong!‚Äù
```

*Handlers are **auto-unregistered** when the `BusSession` exits‚Äîno leaks.*

---

## üìä Observability

Add structured logs with zero boilerplate:

```python
from llmgine.bootstrap import ApplicationBootstrap, ApplicationConfig
config = ApplicationConfig(enable_console_handler=True,
                           enable_file_handler=True,
                           log_level="debug")
await ApplicationBootstrap(config).bootstrap()
```

*All events/commands flow through `ConsoleEventHandler` and `FileEventHandler`
to a timestamped `logs/events_*.jsonl` file.*

---

## üìÅ Repository Layout (abridged)

```
llmgine/
‚îÇ
‚îú‚îÄ engines/            # Turn-key example engines (single-pass, tool chat, ‚Ä¶)
‚îî‚îÄ src/llmgine/
   ‚îú‚îÄ bus/             # Message bus core + sessions
   ‚îú‚îÄ llm/
   ‚îÇ   ‚îú‚îÄ context/     # Chat history & context events
   ‚îÇ   ‚îú‚îÄ engine/      # Engine base + dummy
   ‚îÇ   ‚îú‚îÄ models/      # Provider-agnostic model wrappers
   ‚îÇ   ‚îú‚îÄ providers/   # OpenAI, OpenRouter, Gemini, Dummy, ‚Ä¶
   ‚îÇ   ‚îî‚îÄ tools/       # ToolManager, parser, register, types
   ‚îú‚îÄ observability/   # Console & file handlers, log events
   ‚îî‚îÄ ui/cli/          # Rich-based CLI components
```

---

## üèÅ Roadmap

- [ ] **Streaming responses** with incremental event dispatch  
- [ ] **WebSocket / FastAPI** front-end (drop-in replacement for CLI)  
- [ ] **Persistent vector memory** layer behind `ContextManager`  
- [ ] **Plugin system** for third-party Observability handlers  
- [ ] **More providers**: Anthropic, Vertex AI, etc.

---

## ü§ù Contributing

1. Fork & create a feature branch  
2. Ensure `pre-commit` passes (`ruff`, `black`, `isort`, `pytest`)  
3. Open a PR with context + screenshots/GIFs if UI-related  

---

## üìÑ License

LLMgine is distributed under the **MIT License**‚Äîsee [`LICENSE`](LICENSE) for details.

---

> _‚ÄúBuild architecturally sound LLM apps, not spaghetti code.  
> Welcome to the engine room.‚Äù_


================================================
FILE: programs/__init__.py
================================================
[Empty file]


================================================
FILE: programs/engines/__init__.py
================================================
[Empty file]


================================================
FILE: programs/engines/engine_guide.md
================================================
# Engine Guide

An engine file is split into 3 (or 4) sections:

1. Engine commands
2. Engine class
3. CLI
4 (optional). Engine function

## Engine commands

### CustomEngineCommand

Each engine must have a command that takes in your prompt. 

```python
@dataclass
class CustomEngineCommand(Command):
    prompt: str = ""
    (and any other arguments you need)
```

This command will be used as the entry into your engine. 

### CustomEngineStatusEvent

Each engine must have a status event that is emitted when the engine is started.

```python
@dataclass
class CustomEngineStatusEvent(Event):
    status: str = ""
```

This event will be used to update the status of the engine. This is the loading bar you see in the CLI.

### PromptCommand

A prompt command is a command that gets a prompt from the user. Checkout the prompts class in the CLI for more information. 

```python
@dataclass
class PromptCommand(Command):
    prompt: str = ""
```

There are two defined prompt components, the simple yes or no or the full text prompt. They are in cli.components.py.

### Other Commands and Events

You can add other commands and events to your engine as needed. They will be registered to different components or prompts in the CLI. 

## Engine class

The engine class is where you define your engine. 

The entry into your engine is the handle_command function. Which takes in your CustomEngineCommand and returns a CommandResult. See the CommandResult class in the bus.commands module for more information. 

```python
async def handle_command(self, command: CustomEngineCommand) -> CommandResult:
    pass
```

The core logic of your engine should be in the execute function. 

```python
async def execute(self, prompt: str) -> str:
    pass
```

So your handle_command function will look like this:

```python
async def handle_command(self, command: CustomEngineCommand) -> CommandResult:
    try:
        result = await self.execute(command.prompt)
        return CommandResult(success=True, result=result)
    except Exception as e:
        return CommandResult(success=False, error=str(e))
```

Inside your execute function and your init is where all your custom logic will go.

## CLI

The CLI is defined in the cli.py file. It is a class that is responsible for handling the CLI. It is initialized with your engine and the model you want to use. 

The CLI class is callled 'EngineCLI' and is initialized with your engine's session id.

```python
cli = EngineCLI(session_id)
```

At the bare minimum, you need to register a couple of things for the cli to work.

```python
cli.register_engine(engine) # register the engine
cli.register_engine_command(CustomEngineCommand, engine.handle_command) # register the command to the engine input
cli.register_engine_result_component(EngineResultComponent) # register the result component to the engine output
cli.register_loading_event(CustomEngineStatusEvent) # register the loading event to the engine status
```

From there you can just do:

```python
await cli.main()
```

A full example of an engine looks like this:

```python
    from llmgine.ui.cli.cli import EngineCLI
    from llmgine.ui.cli.components import EngineResultComponent
    from llmgine.bootstrap import ApplicationConfig, ApplicationBootstrap
    from llmgine.llm.models.openai_models import Gpt41Mini
    from llmgine.llm.providers.providers import Providers

    config = ApplicationConfig(enable_console_handler=False)
    bootstrap = ApplicationBootstrap(config)
    await bootstrap.bootstrap()
    engine = SinglePassEngine(
        Gpt41Mini(Providers.OPENAI), "respond in pirate", "test"
    )
    cli = EngineCLI("test")
    cli.register_engine(engine)
    cli.register_engine_command(SinglePassEngineCommand, engine.handle_command)
    cli.register_engine_result_component(EngineResultComponent)
    cli.register_loading_event(SinglePassEngineStatusEvent)
    await cli.main()
```

You would write your cli inside of the engine file under main(), so then you can do if __name__ == "__main__": and run the engine directly.

See single_pass_engine.py for a full example.

###  Engine function

There's an additional step which is to make the engine usable through a single function.

```python
async def custom_engine(prompt: str) -> str:
    pass
```

This would be a function that takes in some value and the sets everything up, executes it and returns the result. 

To test this we put the condition case in the main() function, and we have a case for cli and also a case for the engine function. 

Check single_pass_engine.py for an example.


================================================
FILE: programs/engines/single_pass_engine.py
================================================
import uuid
from dataclasses import dataclass
from typing import Optional

from llmgine.bus.bus import MessageBus
from llmgine.llm import SessionID
from llmgine.llm.engine.engine import Engine
from llmgine.llm.models.model import Model
from llmgine.llm.providers.response import LLMResponse
from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event


@dataclass
class SinglePassEngineCommand(Command):
    prompt: str = ""


@dataclass
class SinglePassEngineStatusEvent(Event):
    status: str = ""


class SinglePassEngine(Engine):
    def __init__(
        self,
        model: Model,
        system_prompt: Optional[str] = None,
        session_id: Optional[SessionID] = None,
    ):
        self.model = model
        self.system_prompt = system_prompt
        self.session_id = session_id
        self.bus = MessageBus()

    async def handle_command(self, command: SinglePassEngineCommand) -> CommandResult:
        try:
            result = await self.execute(command.prompt)
            return CommandResult(success=True, result=result)
        except Exception as e:
            return CommandResult(success=False, error=str(e))

    async def execute(self, prompt: str) -> str:
        if self.system_prompt:
            context = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": prompt},
            ]
        else:
            context = [{"role": "user", "content": prompt}]
        await self.bus.publish(
            SinglePassEngineStatusEvent(status="Calling LLM", session_id=self.session_id)
        )

        response: LLMResponse = await self.model.generate(context)
        await self.bus.publish(
            SinglePassEngineStatusEvent(status="finished", session_id=self.session_id)
        )
        return response.content


async def use_single_pass_engine(
    prompt: str, model: Model, system_prompt: Optional[str] = None
):
    session_id = SessionID(str(uuid.uuid4()))
    engine = SinglePassEngine(model, system_prompt, session_id)
    return await engine.execute(prompt)


async def main(case: int):
    from llmgine.bootstrap import ApplicationBootstrap, ApplicationConfig
    from llmgine.llm.models.openai_models import Gpt41Mini
    from llmgine.llm.providers.providers import Providers
    from llmgine.ui.cli.cli import EngineCLI
    from llmgine.ui.cli.components import EngineResultComponent

    config = ApplicationConfig(enable_console_handler=False)
    bootstrap = ApplicationBootstrap(config)
    await bootstrap.bootstrap()
    if case == 1:
        engine = SinglePassEngine(
            Gpt41Mini(Providers.OPENAI), "respond in pirate", "test"
        )
        cli = EngineCLI("test")
        cli.register_engine(engine)
        cli.register_engine_command(SinglePassEngineCommand, engine.handle_command)
        cli.register_engine_result_component(EngineResultComponent)
        cli.register_loading_event(SinglePassEngineStatusEvent)
        await cli.main()
    elif case == 2:
        result = await use_single_pass_engine(
            "Hello, world!", Gpt41Mini(Providers.OPENAI), "respond in pirate"
        )
        print(result)


if __name__ == "__main__":
    import asyncio

    asyncio.run(main(2))



================================================
FILE: programs/engines/tool_chat_engine.py
================================================
import uuid
import json
import asyncio
from typing import Any

from llmgine.bus.bus import MessageBus
from llmgine.llm.context.memory import SimpleChatHistory
from llmgine.llm.models.openai_models import Gpt41Mini
from llmgine.llm.providers.providers import Providers
from llmgine.llm.tools.tool_manager import ToolManager
from llmgine.llm.tools import ToolCall
from llmgine.llm.models.openai_models import OpenAIResponse
from openai.types.chat.chat_completion_message import ChatCompletionMessage

from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event
from llmgine.ui.cli.cli import EngineCLI
from llmgine.ui.cli.components import EngineResultComponent
from dataclasses import dataclass
from llmgine.llm import SessionID, AsyncOrSyncToolFunction


@dataclass
class ToolChatEngineCommand(Command):
    """Command for the Tool Chat Engine."""

    prompt: str = ""


@dataclass
class ToolChatEngineStatusEvent(Event):
    """Event emitted when the status of the engine changes."""

    status: str = ""


@dataclass
class ToolChatEngineToolResultEVent(Event):
    """Event emitted when a tool is executed."""

    tool_name: str = ""
    result: Any = None


class ToolChatEngine:
    def __init__(
        self,
        session_id: SessionID,
    ):
        """Initialize the LLM engine.

        Args:
            session_id: The session identifier
            api_key: OpenAI API key (defaults to environment variable)
            model: The model to use
            system_prompt: Optional system prompt to set
            message_bus: Optional MessageBus instance (from bootstrap)
        """
        # Use the provided message bus or create a new one
        self.message_bus: MessageBus = MessageBus()
        self.engine_id: str = str(uuid.uuid4())
        self.session_id: SessionID = SessionID(session_id)

        # Create tightly coupled components - pass the simple engine
        self.context_manager = SimpleChatHistory(
            engine_id=self.engine_id, session_id=self.session_id
        )
        self.llm_manager = Gpt41Mini(Providers.OPENAI)
        self.tool_manager = ToolManager(
            engine_id=self.engine_id, session_id=self.session_id, llm_model_name="openai"
        )

    async def handle_command(self, command: ToolChatEngineCommand) -> CommandResult:
        """Handle a prompt command following OpenAI tool usage pattern.

        Args:
            command: The prompt command to handle

        Returns:
            CommandResult: The result of the command execution
        """
        try:
            # 1. Add user message to history
            self.context_manager.store_string(command.prompt, "user")

            # Loop for potential tool execution cycles
            while True:
                # 2. Get current context (including latest user message or tool results)
                current_context = await self.context_manager.retrieve()

                # 3. Get available tools
                tools = await self.tool_manager.get_tools()

                # 4. Call LLM
                # print(
                #     f"\nCalling LLM with context:\n{json.dumps(current_context, indent=2)}\n"
                # )  # Debug print
                await self.message_bus.publish(
                    ToolChatEngineStatusEvent(
                        status="calling LLM", session_id=self.session_id
                    )
                )
                response: OpenAIResponse = await self.llm_manager.generate(
                    messages=current_context, tools=tools
                )
                assert isinstance(response, OpenAIResponse), (
                    "response is not an OpenAIResponse"
                )

                # print(f"\nLLM Raw Response:\n{response.raw}\n")  # Debug print

                # 5. Extract the first choice's message object
                # Important: Access the underlying OpenAI object structure
                response_message: ChatCompletionMessage = response.raw.choices[0].message
                assert isinstance(response_message, ChatCompletionMessage), (
                    "response_message is not a ChatCompletionMessage"
                )

                # 6. Add the *entire* assistant message object to history.
                # This is crucial for context if it contains tool_calls.
                await self.context_manager.store_assistant_message(response_message)

                # 7. Check for tool calls
                if not response_message.tool_calls:
                    # No tool calls, break the loop and return the content
                    final_content = response_message.content or ""

                    # Notify status complete
                    await self.message_bus.publish(
                        ToolChatEngineStatusEvent(
                            status="finished", session_id=self.session_id
                        )
                    )
                    return CommandResult(
                        success=True, result=final_content, session_id=self.session_id
                    )

                # 8. Process tool calls
                for tool_call in response_message.tool_calls:
                    tool_call_obj = ToolCall(
                        id=tool_call.id,
                        name=tool_call.function.name,
                        arguments=tool_call.function.arguments,
                    )
                    try:
                        # Execute the tool
                        await self.message_bus.publish(
                            ToolChatEngineStatusEvent(
                                status="executing tool", session_id=self.session_id
                            )
                        )

                        result = await self.tool_manager.execute_tool_call(tool_call_obj)

                        # Convert result to string if needed for history
                        if isinstance(result, dict):
                            result_str = json.dumps(result)
                        else:
                            result_str = str(result)

                        # Store tool execution result in history
                        self.context_manager.store_tool_call_result(
                            tool_call_id=tool_call_obj.id,
                            name=tool_call_obj.name,
                            content=result_str,
                        )

                        # Publish tool execution event
                        await self.message_bus.publish(
                            ToolChatEngineToolResultEVent(
                                tool_name=tool_call_obj.name,
                                result=result_str,
                                session_id=self.session_id,
                            )
                        )

                    except Exception as e:
                        error_msg = f"Error executing tool {tool_call_obj.name}: {str(e)}"
                        print(error_msg)  # Debug print
                        # Store error result in history
                        self.context_manager.store_tool_call_result(
                            tool_call_id=tool_call_obj.id,
                            name=tool_call_obj.name,
                            content=error_msg,
                        )
                # After processing all tool calls, loop back to call the LLM again
                # with the updated context (including tool results).

        except Exception as e:
            # Log the exception before returning
            # logger.exception(f"Error in handle_prompt_command for session {self.session_id}") # Requires logger setup
            print(f"ERROR in handle_prompt_command: {e}")  # Simple print for now
            import traceback

            traceback.print_exc()  # Print stack trace

            return CommandResult(success=False, error=str(e), session_id=self.session_id)

    async def register_tool(self, function: AsyncOrSyncToolFunction):
        """Register a function as a tool.

        Args:
            function: The function to register as a tool
        """
        await self.tool_manager.register_tool(function)
        print(f"Tool registered: {function.__name__}")

    async def clear_context(self):
        """Clear the conversation context."""
        self.context_manager.clear()

    def set_system_prompt(self, prompt: str):
        """Set the system prompt.

        Args:
            prompt: The system prompt to set
        """
        self.context_manager.set_system_prompt(prompt)


async def main():
    import os

    print(f"Current working directory: {os.getcwd()}")

    from tools.test_tools import get_weather
    from llmgine.ui.cli.components import ToolComponent
    from llmgine.bootstrap import ApplicationBootstrap, ApplicationConfig

    config = ApplicationConfig(enable_console_handler=False)
    bootstrap = ApplicationBootstrap(config)
    await bootstrap.bootstrap()

    cli = EngineCLI(SessionID("test"))
    engine = ToolChatEngine(session_id=SessionID("test"))
    await engine.register_tool(get_weather)
    cli.register_engine(engine)
    cli.register_engine_command(ToolChatEngineCommand, engine.handle_command)
    cli.register_engine_result_component(EngineResultComponent)
    cli.register_loading_event(ToolChatEngineStatusEvent)
    cli.register_component_event(ToolChatEngineToolResultEVent, ToolComponent)
    await cli.main()


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: programs/engines/voice_processing_engine.py
================================================
"""
This engine's job is to receive facts and decides whether to
create, update, or delete a fact.

To create or update a fact, construct the content as follows:
<CREATE_FACT><fact>

To delete a fact, construct the content as follows:
<DELETE_FACT><fact>
"""

from typing import Optional
import uuid
import json
from dataclasses import dataclass

from llmgine.llm.engine.engine import Engine
from llmgine.llm.models.model import Model
from llmgine.messages.commands import CommandResult, Command
from llmgine.bus.bus import MessageBus
from llmgine.messages.events import Event
from llmgine.llm.tools.tool_manager import ToolManager
from llmgine.llm.models.openai_models import Gpt41Mini
from llmgine.llm.providers.providers import Providers
from llmgine.llm.context.memory import SimpleChatHistory
from llmgine.llm.tools import ToolCall
from llmgine.ui.cli.voice_processing_engine_cli import (
    SpecificPrompt,
    SpecificComponent,
    SpecificPromptCommand,
    SpecificComponentEvent,
)
from llmgine.llm import SessionID, AsyncOrSyncToolFunction
from programs.stt import process_audio, merge_speakers, merge_speakers_engine
from llmgine.llm.models.openai_models import OpenAIResponse
from openai.types.chat.chat_completion_message import ChatCompletionMessage

SYSTEM_PROMPT = (
    f"You are a voice processing engine. You are provided with the number of speakers inside the conversation, "
    f"and a snippet of what each speaker said in the conversation. "
    f"The number of speakers present in the snippet will be greater than the actual number of speakers in the conversation. "
    f"Your task is to decide which speakers in the snippet should be merged into a single speaker, based on the context, speaking style, "
    f"and the content of what they said. Make sure the number of speakers after merge is the same as the actual number of speakers in the conversation. "
    f"If you think speaker_1 and speaker_2 are actually one person, speaker_3 and speaker_4 are one person: "
    f'example function call: merge_speakers("speaker_1,speaker_2") ; merge_speakers("speaker_3,speaker_4")'
)


@dataclass
class VoiceProcessingEngineCommand(Command):
    prompt: str = ""


@dataclass
class VoiceProcessingEngineStatusEvent(Event):
    status: str = ""


@dataclass
class VoiceProcessingEngineToolResultEvent(Event):
    tool_name: str = ""
    result: str = ""


# ------------------------------------ENGINE-------------------------------------------


class VoiceProcessingEngine(Engine):
    def __init__(
        self,
        model: Model,  # TODO This name and class could be more descriptive
        system_prompt: Optional[str] = None,
        session_id: SessionID = SessionID("test"),
    ):
        self.model: Model = model
        self.system_prompt: Optional[str] = system_prompt
        self.session_id: SessionID = SessionID(session_id)
        self.message_bus: MessageBus = MessageBus()
        self.engine_id: str = str(uuid.uuid4())

        # Create tightly coupled components - pass the simple engine
        self.context_manager = SimpleChatHistory(
            engine_id=self.engine_id, session_id=self.session_id
        )
        self.llm_manager = Gpt41Mini(Providers.OPENAI)
        self.tool_manager = ToolManager(
            engine_id=self.engine_id, session_id=self.session_id, llm_model_name="openai"
        )

    async def handle_command(
        self, command: VoiceProcessingEngineCommand
    ) -> CommandResult:
        """Handle a prompt command following OpenAI tool usage pattern.

        Args:
            command: The prompt command to handle

        Returns:
            CommandResult: The result of the command execution
        """
        try:
            # Process the audio file and get the snippet
            audio_file, number_of_speakers = command.prompt.split("&")
            snippet, audio_file_path = process_audio(audio_file, number_of_speakers)
            self.audio_file_path = audio_file_path

            if len(snippet) == int(number_of_speakers):
                return CommandResult(success=True, result="No merge is required.")

            # Prompt the LLM with the actual number of speakers and the snippet
            prompt = (
                "Actual Number of speakers: "
                + number_of_speakers
                + ".\nHere is the snippet of what each speaker said in the conversation: "
                + str(snippet)
            )
            result = await self.execute(prompt=prompt)

            return CommandResult(success=True, result=result)
        except Exception as e:
            return CommandResult(success=False, error=str(e))

    async def execute(self, prompt: str) -> str:
        """This function executes the engine.

        Args:
            prompt: The prompt to execute
        """

        self.context_manager.store_string(prompt, "user")

        while True:
            # Retrieve the current context
            current_context = await self.context_manager.retrieve()
            # Get the tools
            tools = await self.tool_manager.get_tools()
            # Notify status
            await self.message_bus.publish(
                VoiceProcessingEngineStatusEvent(
                    status="calling LLM", session_id=self.session_id
                )
            )
            # Generate the response
            response: OpenAIResponse = await self.llm_manager.generate(
                messages=current_context, tools=tools, tool_choice="auto"
            )
            assert isinstance(response, OpenAIResponse), (
                "response is not an OpenAIResponse"
            )

            # Get the response message
            response_message: ChatCompletionMessage = response.raw.choices[0].message
            assert isinstance(response_message, ChatCompletionMessage), (
                "response_message is not a ChatCompletionMessage"
            )

            # Store the response message
            await self.context_manager.store_assistant_message(response_message)
            # If there are no tool calls, break the loop and return the content
            if not response_message.tool_calls:
                final_content = response_message.content or ""
                # Notify status complete
                await self.message_bus.publish(
                    VoiceProcessingEngineStatusEvent(
                        status="finished", session_id=self.session_id
                    )
                )
                return final_content

            # Else, process tool calls
            for tool_call in response_message.tool_calls:
                tool_call_obj = ToolCall(
                    id=tool_call.id,
                    name=tool_call.function.name,
                    arguments=tool_call.function.arguments,
                )
                try:
                    # Execute the tool
                    await self.message_bus.publish(
                        VoiceProcessingEngineStatusEvent(
                            status="executing tool", session_id=self.session_id
                        )
                    )

                    # Insert audio file path here manually
                    if tool_call.function.name == "merge_speakers":
                        args = json.loads(tool_call.function.arguments)
                        args["audio_file"] = self.audio_file_path
                        tool_call_obj.arguments = json.dumps(args)
                        tool_call_obj.name = "merge_speakers_engine"

                    result = await self.tool_manager.execute_tool_call(tool_call_obj)

                    # Convert result to string if needed for history
                    if isinstance(result, dict):
                        result_str = json.dumps(result)
                    else:
                        result_str = str(result)
                    # Store tool execution result in history
                    self.context_manager.store_tool_call_result(
                        tool_call_id=tool_call_obj.id,
                        name=tool_call_obj.name,
                        content=result_str,
                    )
                    # Publish tool execution event
                    await self.message_bus.publish(
                        VoiceProcessingEngineToolResultEvent(
                            tool_name=tool_call_obj.name,
                            result=result_str,
                            session_id=self.session_id,
                        )
                    )

                except Exception as e:
                    error_msg = f"Error executing tool {tool_call_obj.name}: {str(e)}"
                    print(error_msg)  # Debug print
                    # Store error result in history
                    self.context_manager.store_tool_call_result(
                        tool_call_id=tool_call_obj.id,
                        name=tool_call_obj.name,
                        content=error_msg,
                    )

    async def register_tool(self, function: AsyncOrSyncToolFunction):
        """Register a function as a tool.

        Args:
            function: The function to register as a tool
        """
        await self.tool_manager.register_tool(function)


async def main():
    from llmgine.ui.cli.voice_processing_engine_cli import VoiceProcessingEngineCLI
    from llmgine.ui.cli.components import EngineResultComponent, ToolComponent
    from llmgine.bootstrap import ApplicationConfig, ApplicationBootstrap
    from llmgine.llm.models.openai_models import Gpt41Mini
    from llmgine.llm.providers.providers import Providers

    config = ApplicationConfig(enable_console_handler=False)
    bootstrap = ApplicationBootstrap(config)
    await bootstrap.bootstrap()

    # Initialize the engine
    engine = VoiceProcessingEngine(
        model=Gpt41Mini(Providers.OPENAI),
        system_prompt=SYSTEM_PROMPT,
        session_id=SessionID("test"),
    )

    # Register cli components
    cli = VoiceProcessingEngineCLI("voice processing engine")
    cli.register_engine(engine)
    cli.register_engine_command(VoiceProcessingEngineCommand, engine.handle_command)
    cli.register_engine_result_component(EngineResultComponent)
    cli.register_loading_event(VoiceProcessingEngineStatusEvent)
    cli.register_component_event(VoiceProcessingEngineToolResultEvent, ToolComponent)
    cli.register_prompt_command(SpecificPromptCommand, SpecificPrompt)
    cli.register_component_event(SpecificComponentEvent, SpecificComponent)

    # Register tools
    await engine.register_tool(merge_speakers)
    await engine.register_tool(merge_speakers_engine)

    await cli.main()


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())



================================================
FILE: src/llmgine/__init__.py
================================================
[Empty file]


================================================
FILE: src/llmgine/bootstrap.py
================================================
"""Bootstrap utilities for application initialization.

Provides a way to bootstrap the application components including
the observability bus and the message bus.
"""

import logging
from dataclasses import dataclass
from typing import Callable, Generic, Optional, Type, TypeVar

from llmgine.bus.bus import MessageBus
from llmgine.bus.session import BusSession
from llmgine.messages.commands import Command
from llmgine.messages.events import Event
from llmgine.observability.events import LogLevel
from llmgine.observability.handlers import (
    ConsoleEventHandler,
    FileEventHandler,
)

logger = logging.getLogger(__name__)

# Type definitions
TConfig = TypeVar("TConfig")


# --- Basic Logging Setup Function ---
def setup_basic_logging(level: LogLevel = LogLevel.INFO):
    """Configure basic Python logging to the console."""
    log_level_map = {
        LogLevel.DEBUG: logging.DEBUG,
        LogLevel.INFO: logging.INFO,
        LogLevel.WARNING: logging.WARNING,
        LogLevel.ERROR: logging.ERROR,
        LogLevel.CRITICAL: logging.CRITICAL,
    }
    logging_level = log_level_map.get(level, logging.INFO)

    # Configure logging with session_id support
    logging.basicConfig(
        level=logging.ERROR,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        # stream=sys.stdout # Optionally direct to stdout instead of stderr
    )

    logger.info(f"Basic logging configured with level {logging_level}")


@dataclass
class ApplicationConfig:
    """Base configuration for applications."""

    # General application config
    name: str = "application"
    description: str = "application description"

    # --- Standard Logging Config ---
    # Controls standard Python logging setup (not MessageBus handlers)
    log_level: LogLevel = LogLevel.INFO

    # --- Observability Handler Config ---
    enable_console_handler: bool = True
    enable_file_handler: bool = True
    file_handler_log_dir: str = "logs"
    file_handler_log_filename: Optional[str] = None  # Default: timestamped events.jsonl
    # custom_handlers: List[ObservabilityEventHandler] = field(default_factory=list) # For adding other handlers


class ApplicationBootstrap(Generic[TConfig]):
    """Bootstrap for application initialization.

    Handles setting up the message bus and registering configured
    observability event handlers.
    """

    def __init__(self, config: TConfig = ApplicationConfig()):
        """Initialize the bootstrap.

        Args:
            config: Application configuration
        """
        self.config = config

        # --- Configure Standard Logging ---
        # Get log level from config, default to INFO
        log_level_config = getattr(self.config, "log_level", LogLevel.INFO)
        setup_basic_logging(level=log_level_config)
        # --- End Logging Config ---

        # --- Initialize MessageBus (now takes no args) ---
        self.message_bus = MessageBus()

    async def bootstrap(self) -> None:
        """Bootstrap the application.

        Starts the message bus, and registers handlers.
        """
        logger.info(
            "Application bootstrap started", extra={"component": "ApplicationBootstrap"}
        )

        # Start message bus
        await self.message_bus.start()

        # Register command and event handlers
        self._register_observability_handlers()
        self._register_command_handlers()
        self._register_event_handlers()

        logger.info(
            "Application bootstrap completed", extra={"component": "ApplicationBootstrap"}
        )

    async def shutdown(self) -> None:
        """Shutdown the application components."""
        # Close the primary session (using __aexit__ since it's an async context manager)
        if hasattr(self, "primary_session") and self.primary_session._active:
            await self.primary_session.__aexit__(None, None, None)

        # Stop message bus
        await self.message_bus.stop()

        logger.info(
            "Application shutdown complete", extra={"component": "ApplicationBootstrap"}
        )

    def _register_observability_handlers(self) -> None:
        """Register observability handlers with the message bus."""
        if self.config.enable_console_handler:
            self.message_bus.register_observability_handler(ConsoleEventHandler())
        if self.config.enable_file_handler:
            self.message_bus.register_observability_handler(FileEventHandler())

    def _register_command_handlers(self) -> None:
        """Register command handlers with the message bus.

        Override this method to register your engine's command handlers.
        """
        pass

    def _register_event_handlers(self) -> None:
        """Register event handlers with the message bus.

        Override this method to register your engine's event handlers.
        """
        pass

    def register_command_handler(
        self, command_type: Type[Command], handler: Callable
    ) -> None:
        """Register a command handler with the message bus.

        Args:
            command_type: The type of command to handle
            handler: The function that handles the command
        """
        # Use the primary session as the default
        self.primary_session.register_command_handler(command_type, handler)

    def register_event_handler(self, event_type: Type[Event], handler: Callable) -> None:
        """Register an event handler with the message bus.

        Args:
            event_type: The type of event to handle
            handler: The function that handles the event
        """
        # Use the primary session as the default
        self.primary_session.register_event_handler(event_type, handler)

    def create_session(self) -> BusSession:
        """Create a new session for session-specific handlers.

        Returns:
            A new BusSession that can be used as a context manager
        """
        return self.message_bus.create_session()


class CommandBootstrap(ApplicationBootstrap[TConfig]):
    """Legacy bootstrap class for backward compatibility."""

    pass



================================================
FILE: src/llmgine/py.typed
================================================




================================================
FILE: src/llmgine/bus/README.md
================================================
# Message Bus and Sessions

This package provides a message bus implementation for handling commands and events in the application.

## Message Bus

The MessageBus is a central communication mechanism in the application, providing a way for components to communicate without direct dependencies.

The bus follows these patterns:
- **Command Bus**: Commands represent operations to be performed and are handled by exactly one handler
- **Event Bus**: Events represent things that have happened and can be processed by multiple listeners
- **Observability**: All operations are traced for debugging and monitoring

## Sessions

Sessions allow grouping related operations together:
- Each session has a unique ID
- Commands and events can be associated with a session
- When a session ends, all its handlers are automatically unregistered

## Usage Example

Here's how to use sessions:

```python
from llmgine.bus.bus import MessageBus
from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event
import asyncio

# Define an event
class MyEvent(Event):
    def __init__(self, message: str):
        super().__init__()
        self.message = message

# Define a command
class MyCommand(Command):
    def __init__(self, data: str):
        super().__init__()
        self.data = data

# Define event handler
def handle_my_event(event: MyEvent) -> None:
    print(f"Handling event: {event.message}")
    
# Define command handler  
def handle_my_command(command: MyCommand) -> CommandResult:
    print(f"Executing command: {command.data}")
    return CommandResult(success=True, original_command=command)

async def main():
    # Get the message bus
    bus = MessageBus()
    
    # Start a session using the context manager
    with bus.create_session() as session:
        # Register handlers with this session
        session.register_event_handler(MyEvent, handle_my_event)
        session.register_command_handler(MyCommand, handle_my_command)
        
        # Execute a command in this session
        command = MyCommand("Hello from command")
        result = await session.execute_with_session(command)
        
        # Publish an event in this session
        event = MyEvent("Hello from event")
        event.session_id = session.session_id  # Set session ID
        await bus.publish(event)
        
        # When the session exits:
        # 1. All handlers registered with this session are unregistered
        # 2. A SessionEndEvent is published
    
    # After session ends, the handlers are no longer active

if __name__ == "__main__":
    asyncio.run(main())
```

## Advantages of the Session Pattern

- **Cleanup**: Automatic cleanup of handlers when a session ends
- **Context**: Operations can be grouped and traced together
- **Isolation**: Different sessions can operate independently
- **Tracing**: All operations within a session can be traced together 


================================================
FILE: src/llmgine/bus/__init__.py
================================================
"""Message bus components for the LLMgine system."""

from llmgine.bus.bus import MessageBus
from llmgine.bus.fakes import FakeMessageBus

__all__ = ["FakeMessageBus", "MessageBus"]



================================================
FILE: src/llmgine/bus/bus.py
================================================
"""Core message bus implementation for handling commands and events.

The message bus is the central communication mechanism in the application,
providing a way for components to communicate without direct dependencies.
"""

import json
import os
import sys
import asyncio
import contextvars
from datetime import datetime
import logging
import traceback
from types import TracebackType
from typing import (
    Any,
    Awaitable,
    Callable,
    Dict,
    List,
    Optional,
    Type,
    TypeVar,
    Union,
    cast,
)

from llmgine.bus.session import BusSession
from llmgine.bus.utils import is_async_function
from llmgine.llm import SessionID
from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import (
    CommandResultEvent,
    CommandStartedEvent,
    Event,
    EventHandlerFailedEvent,
    ScheduledEvent,
)
from llmgine.observability.handlers.base import ObservabilityEventHandler

# Get the base logger and wrap it with the adapter
logger = logging.getLogger(__name__)

# TODO: add tracing and span context using contextvars
trace: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar(
    "trace", default=None
)
span: contextvars.ContextVar[Optional[str]] = contextvars.ContextVar("span", default=None)

# Type variables for command and event handlers
CommandType = TypeVar("CommandType", bound=Command)
EventType = TypeVar("EventType", bound=Event)
CommandHandler = Callable[[Command], CommandResult]
AsyncCommandHandler = Callable[[Command], Awaitable[CommandResult]]
EventHandler = Callable[[Event], None]
AsyncEventHandler = Callable[[Event], Awaitable[None]]


# Combined types

AsyncOrSyncCommandHandler = Union[AsyncCommandHandler, CommandHandler]


class MessageBus:
    """Async message bus for command and event handling (Singleton).

    This is a simplified implementation of the Command Bus and Event Bus patterns,
    allowing for decoupled communication between components.
    """

    # --- Singleton Pattern ---
    _instance: Optional["MessageBus"] = None

    def __new__(cls, *args: Any, **kwargs: Any) -> "MessageBus":
        """
        Ensure only one instance is created (Singleton pattern).
        """
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance

    def __init__(self) -> None:
        """
        Initialize the message bus (only once).
        Sets up handler storage, event queue, and observability handlers.
        """
        if getattr(self, "_initialized", False):
            return

        self._command_handlers: Dict[
            SessionID, Dict[Type[Command], AsyncCommandHandler]
        ] = {}
        self._event_handlers: Dict[
            SessionID, Dict[Type[Event], List[AsyncEventHandler]]
        ] = {}
        self._event_queue: Optional[asyncio.Queue[Event]] = None
        self._processing_task: Optional[asyncio.Task[None]] = None
        self._observability_handlers: List[ObservabilityEventHandler] = []
        self._suppress_event_errors: bool = True
        self.event_handler_errors: List[Exception] = []
        logger.info("MessageBus initialized")
        self._initialized = True
        self.data_dir = os.path.dirname(os.path.abspath(__file__))

    async def reset(self) -> None:
        """
        Stops the bus if running. Reset the message bus to its initial state.
        """
        await self.stop()
        self.__init__()
        logger.info("MessageBus reset")

    def suppress_event_errors(self) -> None:
        """
        Surpress errors during event handling.
        """
        self._suppress_event_errors = True

    def unsuppress_event_errors(self) -> None:
        """
        Unsupress errors during event handling.
        """
        self._suppress_event_errors = False

        """
        Register an observability handler for this message bus.
        Registers the handler for both general and specific observability events.
        """

    def create_session(self, id_input: Optional[str] = None) -> BusSession:
        """
        Create a new session for grouping related commands and events.
        Args:
            id: Optional session identifier. If not provided, one will be generated.
        Returns:
            A new BusSession instance.
        """
        return BusSession(id=id_input)

    async def start(self) -> None:
        """
        Start the message bus event processing loop.
        Creates the event queue and launches the event processing task if not already running.
        """
        if self._processing_task is None:
            if self._event_queue is None:
                self._event_queue = asyncio.Queue()
                logger.info("Event queue created")
            await self._load_queue()
            self._processing_task = asyncio.create_task(self._process_events())
            logger.info("MessageBus started")
        else:
            logger.warning("MessageBus already running")

    async def stop(self) -> None:
        """
        Stop the message bus event processing loop.
        Cancels the event processing task and cleans up.
        """
        if self._processing_task:
            logger.info("Stopping message bus...")
            self._processing_task.cancel()
            
            await self._dump_queue()
            self._event_queue = None

            try:
                await asyncio.wait_for(self._processing_task, timeout=2.0)
                logger.info("MessageBus stopped successfully")
            except asyncio.CancelledError:
                logger.info("MessageBus task cancelled successfully")
            except Exception as e:
                logger.exception(f"Error during MessageBus shutdown: {e}")
            finally:
                self._processing_task = None
        else:
            logger.info("MessageBus already stopped or never started")

    def register_observability_handler(self, handler: ObservabilityEventHandler) -> None:
        """
        Register an observability handler for a specific session.
        """
        # TODO: add tracing and span
        # TODO: add option to await or not await
        self._observability_handlers.append(handler)

    def register_command_handler(
        self,
        command_type: Type[CommandType],
        handler: AsyncOrSyncCommandHandler,
        session_id: str = "ROOT",
    ) -> None:
        """
        Register a command handler for a specific command type and session.
        Args:
            session_id: The session identifier (or 'ROOT').
            command_type: The type of command to handle.
            handler: The handler function/coroutine.
        Raises:
            ValueError: If a handler is already registered for the command in this session.
        """

        if session_id not in self._command_handlers:
            self._command_handlers[SessionID(session_id)] = {}

        if not is_async_function(handler):
            handler = self._wrap_command_handler_as_async(cast(CommandHandler, handler))

        if command_type in self._command_handlers[SessionID(session_id)]:
            raise ValueError(
                f"Command handler for {command_type} already registered in session {session_id}"
            )

        self._command_handlers[SessionID(session_id)][command_type] = cast(
            AsyncCommandHandler, handler
        )
        logger.debug(
            f"Registered command handler for {command_type} in session {session_id}"
        )  # TODO test

    def register_event_handler(
        self,
        event_type: Type[EventType],
        handler: Union[AsyncEventHandler, EventHandler],
        session_id: SessionID = SessionID("ROOT"),
    ) -> None:
        """
        Register an event handler for a specific event type and session.
        Args:
            session_id: The session identifier (or 'ROOT').
            event_type: The type of event to handle.
            handler: The handler function/coroutine.
        """
        session_id = session_id or SessionID(
            "ROOT"
        )  # TODO is this needed? with default arg?

        if session_id not in self._event_handlers:
            self._event_handlers[SessionID(session_id)] = {}

        if event_type not in self._event_handlers[SessionID(session_id)]:
            self._event_handlers[SessionID(session_id)][event_type] = []

        if not is_async_function(handler):
            handler = self._wrap_event_handler_as_async(cast(EventHandler, handler))

        self._event_handlers[SessionID(session_id)][event_type].append(
            cast(AsyncEventHandler, handler)
        )
        logger.debug(f"Registered event handler for {event_type} in session {session_id}")

    def unregister_session_handlers(self, session_id: SessionID) -> None:
        """
        Unregister all command and event handlers for a specific session.
        Args:
            session_id: The session identifier.
        """
        if session_id not in self._command_handlers:
            logger.debug(f"No command handlers to unregister for session {session_id}")
            return

        if session_id in self._command_handlers:
            num_cmd_handlers = len(self._command_handlers[session_id])
            del self._command_handlers[session_id]
            logger.debug(
                f"Unregistered {num_cmd_handlers} command handlers for session {session_id}"
            )

        if session_id in self._event_handlers:
            num_event_handlers = sum(
                len(handlers) for handlers in self._event_handlers[session_id].values()
            )
            del self._event_handlers[session_id]
            logger.debug(
                f"Unregistered {num_event_handlers} event handlers for session {session_id}"
            )

    def unregister_command_handler(
        self, command_type: Type[CommandType], session_id: str = "ROOT"
    ) -> None:
        """
        Unregister a command handler for a specific command type and session.
        Args:
            command_type: The type of command.
            session_id: The session identifier (default 'ROOT').
        """
        if session_id in self._command_handlers:
            if command_type in self._command_handlers[session_id]:
                del self._command_handlers[session_id][command_type]
                logger.debug(
                    f"Unregistered command handler for {command_type} in session {session_id}"
                )
        else:
            raise ValueError(
                f"No command handlers to unregister for session {session_id}"
            )

    def unregister_event_handlers(
        self, event_type: Type[EventType], session_id: SessionID = SessionID("ROOT")
    ) -> None:
        """
        Unregister an event handler for a specific event type and session.
        Args:
            event_type: The type of event.
            session_id: The session identifier (default 'ROOT').
        """
        if session_id in self._event_handlers:
            if event_type in self._event_handlers[session_id]:
                del self._event_handlers[session_id][event_type]
                logger.debug(
                    f"Unregistered event handler for {event_type} in session {session_id}"
                )
        else:
            raise ValueError(f"No event handlers to unregister for session {session_id}")

    # --- Command Execution and Event Publishing ---

    async def execute(self, command: Command) -> CommandResult:
        """
        Execute a command and return its result.
        Args:
            command: The command instance to execute.
        Returns:
            CommandResult: The result of command execution.
        Raises:
            ValueError: If no handler is registered for the command type.
        """
        command_type = type(command)
        if command.session_id is None:
            raise ValueError("Command has no session ID")

        handler = None
        if command.session_id in self._command_handlers:
            handler = self._command_handlers[command.session_id].get(command_type)

        # Default to ROOT handlers if no session-specific handler is found
        if handler is None and SessionID("ROOT") in self._command_handlers:
            handler = self._command_handlers[SessionID("ROOT")].get(command_type)
            logger.warning(
                f"Defaulting to ROOT command handler for {command_type.__name__} in session {command.session_id}"
            )

        if handler is None:
            logger.error(
                f"No handler registered for command type {command_type.__name__}"
            )
            raise ValueError(f"No handler registered for command {command_type.__name__}")

        try:
            logger.info(f"Executing command {command_type.__name__}")
            await self.publish(
                CommandStartedEvent(command=command, session_id=command.session_id)
            )
            result: CommandResult = await handler(command)
            logger.info(f"Command {command_type.__name__} executed successfully")
            await self.publish(
                CommandResultEvent(command_result=result, session_id=command.session_id)
            )
            return result

        except Exception as e:
            logger.exception(f"Error executing command {command_type.__name__}: {e}")
            failed_result = CommandResult(
                success=False,
                command_id=command.command_id,
                error=f"{type(e).__name__}: {e!s}",
                metadata={"exception_details": traceback.format_exc()},
            )
            await self.publish(CommandResultEvent(command_result=failed_result))
            return failed_result

    async def publish(self, event: Event, await_processing: bool = True) -> None:
        """
        Publish an event onto the event queue.
        Args:
            event: The event instance to publish.
        """

        logger.info(
            f"Publishing event {type(event).__name__} in session {event.session_id}"
        )

        try:
            if self._event_queue is None:
                raise ValueError("Event queue is not initialized")
            await self._event_queue.put(event)
            logger.debug(f"Queued event: {type(event).__name__}")
        except Exception as e:
            logger.error(f"Error queing event: {e}", exc_info=True)
        finally:
            if not isinstance(event, ScheduledEvent) and await_processing:
                await self.ensure_events_processed()

    async def _process_events(self) -> None:
        """
        Process events from the queue indefinitely.
        Handles each event by dispatching to registered handlers.
        """
        logger.info("Event processing loop starting")

        while True:
            try:
                total_events = self._event_queue.qsize() # type: ignore
                while not self._event_queue.empty() and total_events > 0: # type: ignore
                    try:
                        total_events -= 1
                        event = await self._event_queue.get()  # type: ignore
                        logger.debug(f"Dequeued event {type(event).__name__}")

                        # if a scheduled event is not yet due, we queue it again
                        if isinstance(event, ScheduledEvent) and event.scheduled_time > datetime.now():
                            await self._event_queue.put(event) # type: ignore
                            logger.debug(f"Event {type(event).__name__} is scheduled for {event.scheduled_time}, queuing again")
                            continue

                        try:
                            await self._handle_event(event)
                        except asyncio.CancelledError:
                            logger.warning("Event handling cancelled")
                            raise
                        except Exception:
                            logger.exception(f"Error processing event {type(event).__name__}")
                        finally:
                            self._event_queue.task_done()  # type: ignore
                    except asyncio.CancelledError:
                        logger.info("Event processing loop cancelled")
                        raise
                
                # Leave time for other processes to run
                await asyncio.sleep(1)

            except asyncio.CancelledError:
                logger.info("Event processing loop cancelled")
                raise
            except Exception as e:
                logger.exception(f"Error in event processing loop: {e}")
                await asyncio.sleep(0.1)


    async def ensure_events_processed(self) -> None:
        """
        Ensure all non-scheduled events in the queue are processed.
        """
        if self._event_queue is None:
            return

        temp_queue: List[Event] = []
        while not self._event_queue.empty():
            event = await self._event_queue.get()
            if isinstance(event, ScheduledEvent) and event.scheduled_time > datetime.now():
                temp_queue.append(event)  # Save scheduled events for re-queuing
            else:
                await self._handle_event(event)
        # Re-queue scheduled events
        for event in temp_queue:
            await self._event_queue.put(event)

    async def _handle_event(self, event: Event) -> None:
        """
        Handle a single event by calling all registered handlers.
        Args:
            event: The event instance to handle.
        """
        event_type = type(event)

        handlers = []
        # handle session specific handlers
        if event.session_id in self._event_handlers and event.session_id != "ROOT":
            if event_type in self._event_handlers[event.session_id]:
                handlers.extend(self._event_handlers[event.session_id][event_type])  # type: ignore

            # Default to ROOT handlers if no session-specific handler is found
        elif event.session_id != "ROOT":
            # there is no session in event, so we use ROOT handlers if possible
            if SessionID("ROOT") in self._event_handlers:
                # there is root handlers, so we use them
                if event_type in self._event_handlers[SessionID("ROOT")]:
                    handlers.extend(self._event_handlers[SessionID("ROOT")][event_type])  # type: ignore
                    logger.warning(
                        f"Defaulting to ROOT event handler for {event_type} in session {event.session_id}"
                    )

        # handle root handlers
        if event.session_id == "ROOT" and SessionID("ROOT") in self._event_handlers:
            if event_type in self._event_handlers[SessionID("ROOT")]:
                handlers.extend(self._event_handlers[SessionID("ROOT")][event_type])  # type: ignore

        # Global handlers handle all events
        if SessionID("GLOBAL") in self._event_handlers:
            if event_type in self._event_handlers[SessionID("GLOBAL")]:
                handlers.extend(self._event_handlers[SessionID("GLOBAL")][event_type])  # type: ignore
            logger.info(
                f"Using GLOBAL event handlers {self._event_handlers[SessionID('GLOBAL')]} for {event_type} in session{event.session_id}"
            )

        if not handlers:
            logger.debug(
                f"No non-observability handler registered for event type {event_type}"
            )

        for handler in self._observability_handlers:
            logger.debug(
                f"Dispatching event {event_type} in session {event.session_id} to observability handler {handler.__class__.__name__}"
            )
            try:
                await handler.handle(event)
            except Exception as e:
                logger.exception(
                    f"Error in observability handler {handler.__name__}: {e}"
                )
                if not self._suppress_event_errors:
                    raise e
                else:
                    self.event_handler_errors.append(e)

        logger.debug(
            f"Dispatching event {event_type} in session {event.session_id} to {len(handlers)} handlers"  # type: ignore
        )
        tasks = [asyncio.create_task(handler(event)) for handler in handlers]  # type: ignore
        results = await asyncio.gather(*tasks, return_exceptions=True)  # type: ignore
        for i, result in enumerate(results):  # type: ignore
            if isinstance(result, Exception):
                self.event_handler_errors.append(result)
                handler_name = getattr(handlers[i], "__qualname__", repr(handlers[i]))  # type: ignore
                logger.exception(
                    f"Error in handler '{handler_name}' for {event_type}: {result}"
                )
                if not self._suppress_event_errors:
                    raise result
                else:
                    await self.publish(
                        EventHandlerFailedEvent(
                            event=event, handler=handler_name, exception=result
                        )
                    )

    def _wrap_event_handler_as_async(self, handler: EventHandler) -> AsyncEventHandler:
        async def async_wrapper(event: Event):
            return handler(event)

        async_wrapper.function = handler  # type: ignore[attr-defined]

        return async_wrapper

    def _wrap_command_handler_as_async(
        self, handler: CommandHandler
    ) -> AsyncCommandHandler:
        async def async_wrapper(command: Command):
            return handler(command)

        async_wrapper.function = handler  # type: ignore[attr-defined]

        return async_wrapper

    async def _dump_queue(self) -> None:
        """
        Dump the event queue to a file.
        """
        if self._event_queue is None:
            return
        with open(os.path.join(self.data_dir, "unfinished_events.jsonl"), "w") as f:
            while not self._event_queue.empty():
                event = await self._event_queue.get() # type: ignore
                json.dump(event.to_dict(), f)
                f.write('\n')  # Add newline after each JSON object

    async def _load_queue(self) -> None:
        """
        Load the event queue from a file.
        """
        if self._event_queue is None:
            return
        
        with open(os.path.join(self.data_dir, "unfinished_events.jsonl"), "r") as f:
            for line in f:
                event_dict = json.loads(line)
                if "scheduled_time" in event_dict:
                    event = ScheduledEvent.from_dict(event_dict)
                else:
                    event = Event.from_dict(event_dict)
                await self._event_queue.put(event) # type: ignore

        # Clear the file
        with open(os.path.join(self.data_dir, "unfinished_events.jsonl"), "w") as f:
            f.write("")

def bus_exception_hook(bus: MessageBus) -> None:
    """
    Allows the bus to cleanup when an exception is raised globally.
    """
    def bus_excepthook(exc_type: Type[BaseException], exc_value: BaseException, exc_traceback: TracebackType) -> None:
        logger.info("Global unhandled exception caught by bus excepthook!")
        traceback.print_exception(exc_type, exc_value, exc_traceback)
        
        try:
            # Try to get the current event loop
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # If loop is running, create a task to stop the bus
                loop.create_task(bus.stop())
            else:
                # If loop is not running, run the stop method
                loop.run_until_complete(bus.stop())
        except RuntimeError:
            # No event loop available, try to create one
            try:
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)
                loop.run_until_complete(bus.stop())
                loop.close()
            except Exception as e:
                print(f"Failed to cleanup bus: {e}")
        
        # Exit the program
        sys.exit(1)
    sys.excepthook = bus_excepthook


================================================
FILE: src/llmgine/bus/fakes.py
================================================
"""Fake implementations of the message bus for use in testing."""

from typing import Dict, List, Type

from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event


class FakeMessageBus:
    """A fake message bus implementation for testing.

    This class implements the same interface as the real MessageBus
    but without the asynchronous behavior, making it easier to use in tests.

    This is implemented as a singleton, so only one instance can exist.

    Attributes:
        published_events: A list of events that have been published
        executed_commands: A list of commands that have been executed
    """

    _instance = None

    @classmethod
    def get_instance(cls) -> "FakeMessageBus":
        """Get the singleton instance of FakeMessageBus.

        Returns:
            FakeMessageBus: The singleton instance
        """
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def __init__(self) -> None:
        """Initialize the FakeMessageBus.

        If an instance already exists, raises an exception.
        """
        if FakeMessageBus._instance is not None:
            raise RuntimeError(
                "FakeMessageBus is a singleton - use get_instance() instead"
            )

        self.published_events: List[Event] = []
        self.executed_commands: List[Command] = []
        self._command_handlers: Dict[Type[Command], callable] = {}
        self._event_handlers: Dict[Type[Event], List[callable]] = {}
        FakeMessageBus._instance = self

    @classmethod
    def reset_instance(cls) -> None:
        """Reset the singleton instance.

        This is useful for testing when you want to start with a fresh instance.
        """
        cls._instance = None

    async def start(self) -> None:
        """Start the fake message bus (no-op)."""
        pass

    async def stop(self) -> None:
        """Stop the fake message bus (no-op)."""
        pass

    def register_command_handler(self, command_type, handler) -> None:
        """Register a command handler for testing."""
        self._command_handlers[command_type] = handler

    def register_async_command_handler(self, command_type, handler) -> None:
        """Register an async command handler for testing."""
        self._command_handlers[command_type] = handler

    def register_event_handler(self, event_type, handler) -> None:
        """Register an event handler for testing."""
        if event_type not in self._event_handlers:
            self._event_handlers[event_type] = []
        self._event_handlers[event_type].append(handler)

    def register_async_event_handler(self, event_type, handler):
        """Register an async event handler for testing."""
        if event_type not in self._event_handlers:
            self._event_handlers[event_type] = []
        self._event_handlers[event_type].append(handler)

    async def execute(self, command):
        """Execute a command in the fake bus.

        Records the command and returns a successful result.
        If a handler is registered, it will be called.
        """
        self.executed_commands.append(command)

        if type(command) in self._command_handlers:
            handler = self._command_handlers[type(command)]
            result = handler(command)
            # Handle both sync and async handlers
            if hasattr(result, "__await__"):
                return await result
            return result

        # Default success result
        return CommandResult(success=True)

    async def publish(self, event: Event) -> None:
        """Publish an event to the fake bus.

        Records the event and calls any registered handlers.
        """
        self.published_events.append(event)

        event_type = type(event)
        if event_type in self._event_handlers: # TODO .keys() might be more explicit
            for handler in self._event_handlers[event_type]:
                result = handler(event)
                # Handle both sync and async handlers
                if hasattr(result, "__await__"):
                    await result



================================================
FILE: src/llmgine/bus/session.py
================================================
from dataclasses import dataclass
from typing import Callable, Optional, Type, Any
import asyncio
import time
import uuid
import contextvars

# Import Event directly to avoid circular import
from llmgine.messages.events import Event
from llmgine.messages.commands import Command, CommandResult
from llmgine.llm import SessionID

@dataclass
class SessionEvent(Event):
    """An event that is part of a session."""

    session_id: Optional[SessionID] = None


@dataclass
class SessionStartEvent(SessionEvent):
    """An event that indicates the start of a session."""


@dataclass
class SessionEndEvent(SessionEvent):
    """An event that indicates the end of a session."""

    error: Optional[Exception] = None


class BusSession:
    """An **async** session for the message bus.

    Sessions group related operations and handlers together, allowing for
    automatic cleanup when the session ends.

    Usage:
        async with message_bus.create_session() as session:
            session.register_event_handler(EventType, handler_func)
            # Do work with the session...
            # On exit, all handlers are automatically unregistered
    """

    def __init__(self, id: Optional[str] = None):
        """Initialize a new bus session with a unique ID."""
        # Import MessageBus locally to avoid circular dependency at import time
        from llmgine.bus.bus import MessageBus

        self.session_id = id or str(uuid.uuid4())
        self.start_time = time.time()
        self.bus = MessageBus()
        self._active = True

    async def __aenter__(self):
        """Start the session and publish a session start event."""

        # Publish a session start event and await it
        await self.bus.publish(SessionStartEvent(session_id=self.session_id))
        return self

    async def __aexit__(self, exc_type, exc_value, traceback):
        """Clean up the session, unregistering all handlers and publishing end event."""
        if not self._active:
            return

        try:
            # Unregister all event and command handlers for this session (synchronous)
            self.bus.unregister_session_handlers(self.session_id)

            # Publish session end event and await it
            end_event = SessionEndEvent(
                session_id=self.session_id, error=exc_value if exc_type else None
            )
            await self.bus.publish(end_event)

        finally:
            # Ensure the session is marked inactive even if cleanup fails
            self._active = False

    def register_event_handler(
        self, event_type: Type[Event], handler: Callable[[Event], Any]
    ):
        """Register an event handler for this session.

        Args:
            event_type: The type of event to handle
            handler: The handler function for events of this type
        """
        if not self._active:
            raise RuntimeError("Cannot register handlers on an inactive session")

        # Pass session_id explicitly to the bus registration method
        self.bus.register_event_handler(self.session_id, event_type, handler)
        return self  # For method chaining

    def register_command_handler(
        self, command_type: Type[Command], handler: Callable[[Command], CommandResult]
    ):
        """Register a command handler for this session.

        Args:
            command_type: The type of command to handle
            handler: The handler function for commands of this type
        """
        if not self._active:
            raise RuntimeError("Cannot register handlers on an inactive session")

        # Pass session_id explicitly to the bus registration method
        self.bus.register_command_handler(self.session_id, command_type, handler)
        return self  # For method chaining

    async def execute_with_session(self, command: Command) -> Any:
        """Execute a command with this session's ID.

        This is a helper method that sets the session_id on the command
        before executing it via the message bus and awaits the result.

        Args:
            command: The command to execute

        Returns:
            The result of the command execution
        """
        if not self._active:
            raise RuntimeError("Cannot execute commands on an inactive session")

        # Set session ID on the command
        command.session_id = self.session_id

        # Execute via the bus and await the result directly
        return await self.bus.execute(command)



================================================
FILE: src/llmgine/bus/unfinished_events.jsonl
================================================
[Empty file]


================================================
FILE: src/llmgine/bus/utils.py
================================================
from typing import Any, Callable

import inspect


def is_async_function(function: Callable[..., Any]) -> bool:
    return inspect.iscoroutinefunction(function)




================================================
FILE: src/llmgine/llm/__init__.py
================================================
import asyncio
from typing import Any, Callable, Dict, List, NewType, Union, Literal

# TODO use _type


# Type for tool function
ToolFunction = Callable[..., Any]
AsyncToolFunction = Callable[..., "asyncio.Future[Any]"]
AsyncOrSyncToolFunction = Union[ToolFunction, AsyncToolFunction]


ModelFormattedDictTool = NewType("ModelFormattedDictTool", dict[str, Any])
ContextType = NewType("ContextType", List[Dict[str, Any]])

ModelNameStr = NewType("ModelNameStr", str)

# TODO There is not way this is the right place to put this>
SessionID = NewType("SessionID", str)

LLMConversation = NewType("LLMConversation", List[Dict[str, Any]])

ToolChoiceType = Literal[
    "auto", "none", "required"
]  # TODO an enum would be better, otherwise "none" will have lots of search collisions
ToolChoiceOrDictType = Union[
    ToolChoiceType, Dict[str, Any]
]  # TODO this Dict[str, Any] might be ModelFormattedDictTool? # TODO could rename



================================================
FILE: src/llmgine/llm/context/__init__.py
================================================
"""Context Manager interface and implementations.

This module defines the ContextManager interface and its implementations.
The ContextManager is responsible for managing the context/history for LLM interactions.
"""

from abc import ABC, abstractmethod
from typing import TYPE_CHECKING, Any, Dict, List, Optional

# Import this way to avoid circular imports
if TYPE_CHECKING:
    from llmgine.llm.context.memory import InMemoryContextManager


class ContextManager(ABC):
    """Interface for managing the context/history for LLM interactions."""

    @abstractmethod
    def get_context(self, conversation_id: str) -> List[Dict[str, Any]]:
        """Get the conversation context for a specific conversation.
        
        Args:
            conversation_id: The conversation identifier
            
        Returns:
            List[Dict[str, Any]]: The conversation context/history
        """
        ...

    @abstractmethod
    def add_message(self, conversation_id: str, message: Dict[str, Any]) -> None:
        """Add a message to the conversation context.
        
        Args:
            conversation_id: The conversation identifier
            message: The message to add to the context
        """
        ...

    @abstractmethod
    def clear_context(self, conversation_id: str) -> None:
        """Clear the context for a specific conversation.
        
        Args:
            conversation_id: The conversation identifier
        """
        ...

# Import implementations after the interface definition
from llmgine.llm.context.memory import InMemoryContextManager

__all__ = [
    "ContextManager",
    "InMemoryContextManager",
]



================================================
FILE: src/llmgine/llm/context/context_events.py
================================================
from dataclasses import dataclass, field
from typing import Any, Dict, List

from llmgine.messages.events import Event


@dataclass
class ContextEvent(Event):
    """Base class for all context events."""

    engine_id: str = ""
    context_manager_id: str = ""


@dataclass
class ChatHistoryRetrievedEvent(ContextEvent):
    """Event for when chat history is retrieved."""

    context: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class ChatHistoryUpdatedEvent(ContextEvent):
    """Event for when chat history is updated."""

    context: List[Dict[str, Any]] = field(default_factory=list)



================================================
FILE: src/llmgine/llm/context/memory.py
================================================
"""In-memory implementation of the ContextManager interface."""

import uuid
from typing import Any, Dict, List, Optional

from llmgine.bus.bus import MessageBus
from llmgine.llm import SessionID
from llmgine.llm.context import ContextManager
from llmgine.llm.context.context_events import (
    ChatHistoryRetrievedEvent,
    ChatHistoryUpdatedEvent,
)


class SimpleChatHistory:
    def __init__(self, engine_id: str, session_id: SessionID):
        self.engine_id : str = engine_id
        self.session_id: SessionID = session_id
        self.context_manager_id : str = str(uuid.uuid4())
        self.bus: MessageBus = MessageBus()
        self.response_log: List[Any] = []  # Logs raw responses/inputs
        self.chat_history: List[Dict[str, Any]] = []  # Stores OpenAI formatted messages
        self.system_prompt: Optional[str] = None  # Changed from self.system

    def set_system_prompt(self, prompt: str):
        self.system_prompt = prompt
        # Clear history if system prompt changes?
        # self.clear()

    async def store_assistant_message(self, message_object: Any):
        """Store the raw assistant message object (which might contain tool calls)."""
        self.response_log.append(message_object)
        # Convert the OpenAI message object to the dict format for history
        history_entry : dict[str, Any] = {
            "role": message_object.role,
            "content": message_object.content,
        }
        if message_object.tool_calls:
            history_entry["tool_calls"] = [
                {
                    "id": tc.id,
                    "type": tc.type,
                    "function": {
                        "name": tc.function.name,
                        "arguments": tc.function.arguments,
                    },
                }
                for tc in message_object.tool_calls
            ]
        # Ensure content is not None if there are tool_calls, as per OpenAI spec
        if history_entry.get("tool_calls") and history_entry["content"] is None:
            history_entry["content"] = ""  # Or potentially remove the content key?

        self.chat_history.append(history_entry)
        await self.bus.publish(
            ChatHistoryUpdatedEvent(
                engine_id=self.engine_id,
                session_id=self.session_id,
                context_manager_id=self.context_manager_id,
                context=self.chat_history,
            )
        )

    def store_string(self, string: str, role: str):
        """Store a simple user or system message."""
        self.response_log.append([role, string])
        self.chat_history.append({"role": role, "content": string})

    def store_tool_call_result(self, tool_call_id: str, name: str, content: str):
        """Store the result of a specific tool call."""
        result_message : dict[str, Any] = {
            "role": "tool",
            "tool_call_id": tool_call_id,
            "name": name,
            "content": content,
        }
        self.response_log.append(result_message)  # Log the result message
        self.chat_history.append(result_message)

    async def retrieve(self) -> list[dict[str, Any]]:
        """Retrieve the chat history in OpenAI format."""
        result : list[dict[str, Any]] = []
        if self.system_prompt:
            result.append({"role": "system", "content": self.system_prompt})
        result.extend(self.chat_history)
        await self.bus.publish(
            ChatHistoryRetrievedEvent(
                engine_id=self.engine_id,
                session_id=self.session_id,
                context_manager_id=self.context_manager_id,
                context=result,
            )
        )
        return result

    def clear(self):
        self.response_log = []
        self.chat_history = []
        self.system_prompt = ""


class SingleChatContextManager(ContextManager):
    def __init__(self, max_context_length: int = 100):
        """Initialize the single chat context manager.

        Args:
            max_context_length: Maximum number of messages to keep in context
        """
        self.context_raw : list[dict[str, Any]] = []

    def get_context(self) -> List[Dict[str, Any]]:
        """Get the conversation context for a specific conversation.

        Returns:
            List[Dict[str, Any]]: The conversation context/history
        """
        return self.context_raw

    def add_message(self, message: Dict[str, Any]) -> None:
        """Add a message to the conversation context.

        Args:
            message: The message to add to the context
        """
        self.context_raw.append(message)


class InMemoryContextManager(ContextManager):
    """In-memory implementation of the context manager interface."""

    def __init__(self, max_context_length: int = 100):
        """Initialize the in-memory context manager.

        Args:
            max_context_length: Maximum number of messages to keep in context
        """
        self.contexts: Dict[str, List[Dict[str, Any]]] = {}
        self.max_context_length : int = max_context_length

    def get_context(self, conversation_id: str) -> List[Dict[str, Any]]:
        """Get the conversation context for a specific conversation.

        Args:
            conversation_id: The conversation identifier

        Returns:
            List[Dict[str, Any]]: The conversation context/history
        """
        return self.contexts.get(conversation_id, [])

    def add_message(self, conversation_id: str, message: Dict[str, Any]) -> None:
        """Add a message to the conversation context.

        Args:
        conversation_id: The conversation identifier
            message: The message to add to the context
        """
        if conversation_id not in self.contexts:
            self.contexts[conversation_id] = []

        self.contexts[conversation_id].append(message)

        # Trim context if it exceeds max length
        if len(self.contexts[conversation_id]) > self.max_context_length:
            # Keep the first message (usually system prompt) and trim the oldest messages
            first_message = self.contexts[conversation_id][0]
            self.contexts[conversation_id] = [first_message] + self.contexts[
                conversation_id
            ][-(self.max_context_length - 1) :]

    def clear_context(self, conversation_id: str) -> None:
        """Clear the context for a specific conversation.

        Args:
            conversation_id: The conversation identifier
        """
        if conversation_id in self.contexts:
            self.contexts[conversation_id] = []



================================================
FILE: src/llmgine/llm/engine/__init__.py
================================================
[Empty file]


================================================
FILE: src/llmgine/llm/engine/engine.py
================================================
"""Core LLM Engine for handling interactions with language models."""

import asyncio
from dataclasses import dataclass

from llmgine.bus.bus import MessageBus
from llmgine.llm import SessionID
from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event


class Engine:
    """Placeholder for al engines"""

    pass


@dataclass
class DummyEngineCommand(Command):
    prompt: str = ""


@dataclass
class DummyEngineStatusUpdate(Event):
    status: str = ""


@dataclass
class DummyEngineConfirmationInput(Command):
    prompt: str = ""


@dataclass
class DummyEngineToolResult(Event):
    tool_name: str = ""
    result: str = ""


class DummyEngine(Engine):
    """Dummy engine for testing"""

    def __init__(self, session_id: SessionID):
        self.session_id: SessionID = session_id
        self.bus = MessageBus()

    async def handle_command(self, command: Command):
        result = self.execute(command.prompt)
        await self.bus.publish(
            DummyEngineStatusUpdate(status="started", session_id=self.session_id)
        )
        await asyncio.sleep(1)
        await self.bus.publish(
            DummyEngineStatusUpdate(status="thinking", session_id=self.session_id)
        )
        await asyncio.sleep(1)
        await self.bus.publish(
            DummyEngineStatusUpdate(status="finished", session_id=self.session_id)
        )
        # breakpoint()
        confirmation = await self.bus.execute(
            DummyEngineConfirmationInput(
                prompt="Do you want to execute a tool?", session_id=self.session_id
            )
        )
        await self.bus.publish(
            DummyEngineStatusUpdate(status="executing tool", session_id=self.session_id)
        )
        await asyncio.sleep(1)
        if confirmation.result:
            await self.bus.publish(
                DummyEngineToolResult(
                    tool_name="get_weather",
                    result="Tool result is here!",
                    session_id=self.session_id,
                )
            )
        await self.bus.publish(
            DummyEngineStatusUpdate(status="finished", session_id=self.session_id)
        )
        await self.bus.ensure_events_processed()
        return CommandResult(success=True, result=result)

    def execute(self, prompt: str):
        return "Hello, world!"


def main():
    engine = DummyEngine(SessionID("123"))
    result = engine.handle_command(DummyEngineCommand(prompt="Hello, world!"))
    print(result)


if __name__ == "__main__":
    main()



================================================
FILE: src/llmgine/llm/models/anthropic_models.py
================================================

import os
import uuid
from anthropic import AsyncAnthropic
import dotenv
from pydantic import BaseModel
from typing import List, Dict, Optional, Literal, Union, Any
from llmgine.bootstrap import ApplicationConfig
from llmgine.llm.models.model import Model
from llmgine.llm.providers.anthropic_provider import AnthropicProvider, AnthropicResponse
from llmgine.llm.providers import Providers
from llmgine.llm.providers.providers import Provider
from llmgine.llm.providers.response import LLMResponse
import instructor
from llmgine.llm import ToolChoiceOrDictType, ModelFormattedDictTool

dotenv.load_dotenv()

class Claude35Haiku:
    """
    Claude 3.5 Haiku
    """

    def __init__(self, provider: Providers) -> None:
        self.id = str(uuid.uuid4())
        self.generate = None
        self._setProvider(provider)

    def _setProvider(self, provider: Providers) -> None:
        """Get the provider and set the generate method."""
        if provider == Providers.ANTHROPIC:
            self.api_key = os.getenv("ANTHROPIC_API_KEY")
            self.model = "claude-3-5-haiku-20241022"
            self.provider = AnthropicProvider(
                self.api_key, self.model, self.id
            )
            self.generate = self._generate_from_anthropic
            self.instructor = None
        else:
            raise ValueError(
                f"Provider {provider} not supported for {self.__class__.__name__}"
            )

    async def _generate_from_anthropic(
        self,   
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        temperature: float = 0.7,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        thinking_enabled: bool = False,
        thinking_budget: Optional[int] = None,
        instruct: bool = False,
        response_model: Optional[BaseModel] = None,
        **kwargs,
    ) -> LLMResponse:
        if not instruct:
            result = await self.provider.generate(
                messages=messages,
                tools=tools,
                tool_choice=tool_choice,
                temperature=temperature,
                max_completion_tokens=max_completion_tokens,
                response_format=response_format,
                thinking_enabled=thinking_enabled,
                thinking_budget=thinking_budget,
                **kwargs,
            )
        else:
            if not self.instructor:
                self.instructor = instructor.from_anthropic(AsyncAnthropic())
            result = await self.instructor.messages.generate(
                messages=messages,
                response_model=response_model,
                tools=tools,
                tool_choice=tool_choice,
                temperature=temperature,
                max_completion_tokens=max_completion_tokens,
                **kwargs,
            )
            result = AnthropicResponse(result)
        return result



class HowAmI(BaseModel):
    emotion: str
    reason: str

async def main() -> None:
    import asyncio
    from llmgine.bootstrap import ApplicationBootstrap
    app = ApplicationBootstrap(ApplicationConfig(enable_console_handler=False))
    await app.bootstrap()
    model = Claude35Haiku(Providers.ANTHROPIC)
    response = await model.generate(messages=[{"role": "user", "content": "Hello, how are you?"}], instruct=True, response_model=HowAmI)
    print(response.content)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())



================================================
FILE: src/llmgine/llm/models/gemini_models.py
================================================
import os
import uuid
import dotenv
from typing import List, Dict, Optional, Literal, Union, Any
from llmgine.llm.models.model import Model
from llmgine.llm.providers.openrouter import OpenRouterProvider, OpenRouterResponse
from llmgine.llm.providers import Providers
from llmgine.llm.providers.providers import Provider
from llmgine.llm.providers.response import LLMResponse
from llmgine.llm import ToolChoiceOrDictType, ModelFormattedDictTool

dotenv.load_dotenv()


class Gemini25FlashPreview:
    """
    Gemini 2.5 Flash Preview
    """

    def __init__(self, provider: Providers) -> None:
        self.id : str = str(uuid.uuid4())
        self.generate : Optional[Any] = None
        self._setProvider(provider)

    def _setProvider(self, provider: Providers) -> None:
        """Get the provider and set the generate method."""
        if provider == Providers.OPENROUTER:
            self.api_key = os.getenv("OPENROUTER_API_KEY")
            self.model = "google/gemini-2.5-flash-preview"
            assert self.api_key is not None, "OPENROUTER_API_KEY is not set"
            self.provider = OpenRouterProvider(
                self.api_key, self.model, "Google AI Studio", self.id
            )
            self.generate = self._generate_from_openrouter
        else:
            raise ValueError(
                f"Provider {provider} not supported for {self.__class__.__name__}"
            )

    def _generate_from_openrouter(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        temperature: float = 0.7,
        max_completion_tokens: int = 5068,
    ) -> LLMResponse:
        

        tmp = self.provider.generate(
            messages=messages,
            tools=tools,
            tool_choice=tool_choice,
            temperature=temperature,
            max_completion_tokens=max_completion_tokens,
        )
        #assert isinstance(tmp, LLMResponse), "tmp is not an LLMResponse"
        return tmp




================================================
FILE: src/llmgine/llm/models/model.py
================================================
from typing import Any
from abc import ABC, abstractmethod
from llmgine.llm.providers.response import LLMResponse


class Model(ABC):
    """
    Base class for all models.
    """

    @abstractmethod
    def generate(self, **kwargs: Any) -> LLMResponse:
        """
        Generate a response from the model.
        """
        



================================================
FILE: src/llmgine/llm/models/openai_models.py
================================================
"""
This modules contains the models for the OpenAI API.
Each model contains:
- a provider (provided by the llm manager)
- an api key (from env)
- a base url (uniquely hardcoded)
- a model name

Current supported models:
- GPT-4o Mini
    - openai provider: no restrictions
    - openrouter provider: parallel tool calls restricted
- GPT-o3 Mini: temperature restricted
    - openai provider: no restrictions
    - openrouter provider: parallel tool calls restricted

Current supported providers:
- OpenAI
- OpenRouter
"""

import os
import dotenv
from typing import List, Dict, Optional, Literal, Union, Any
from llmgine.llm.models.model import Model
from llmgine.llm.providers.openai_provider import OpenAIResponse, OpenAIProvider
from llmgine.llm.providers.openrouter import OpenRouterProvider
from llmgine.llm.providers import Providers
from llmgine.llm.tools.tool_parser import ModelFormattedDictTool
from llmgine.llm import ToolChoiceOrDictType

dotenv.load_dotenv(override=True)


class OpenAI_Gpt41:
    """
    The latest GPT-4.1 model.
    """

    def __init__(self) -> None:
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.model = "gpt-4.1-2025-04-14"
        self.provider = OpenRouterProvider(self.api_key, self.model)

    def generate(self, messages: List[Dict[str, Any]], **kwargs: Any) -> OpenAIResponse:
        """
        Generate a response from the GPT-4.1 model.
        """
        return self.provider.generate(messages, **kwargs)


class Gpt41:
    """
    The latest GPT-4.1 model.
    """

    def __init__(self, provider: Providers) -> None:
        self.generate = None
        self.model = "gpt-4.1-2025-04-14"
        self.api_key = os.getenv("OPENAI_API_KEY")
        self.provider = self.__getProvider(provider)

    def __getProvider(self, provider: Providers) -> OpenAIProvider:
        """Get the provider and set the generate method."""
        if provider == Providers.OPENROUTER:
            self.generate = self._generate_openrouter
            return OpenRouterProvider(self.api_key, self.model)
        elif provider == Providers.OPENAI:
            self.generate = self._generate_openai
            return OpenAIProvider(self.api_key, self.model)
        else:
            raise ValueError(
                f"Provider {provider} not supported for {self.__class__.__name__}"
            )

    def _generate_openai(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will hardcode a group of default parameters for the OpenAI provider for the GPT-4o Mini model.
        """
        # Update the parameters with the ones provided in the kwargs.
        tmp = self.provider.generate(
            messages=messages,
            temperature=temperature,
            tools=tools,
            tool_choice=tool_choice,
            parallel_tool_calls=parallel_tool_calls,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=None,
            test=test,
            **kwargs,
        )
        assert isinstance(tmp, OpenAIResponse), "tmp is not an OpenAIResponse"
        return tmp

    def _generate_openrouter(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will construct a default group of parameters for the OpenRouter provider.
        """
        tmp = self.provider.generate(
            messages=messages,
            temperature=temperature,
            tools=tools,
            tool_choice=tool_choice,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            test=test,
            **kwargs,
        )
        assert isinstance(tmp, OpenAIResponse)
        return tmp


class Gpt41Mini:
    """
    The latest GPT-4.1 Mini model.
    """

    def __init__(self, provider: Providers) -> None:
        self.generate = None
        self.model: str = "gpt-4.1-mini-2025-04-14"
        self.api_key: str = os.getenv("OPENAI_API_KEY")
        assert self.api_key is not None, "OPENAI_API_KEY is not set"

        self.provider = self.__getProvider(provider)

    def __getProvider(self, provider: Providers) -> OpenAIProvider:
        """Get the provider and set the generate method."""
        if provider == Providers.OPENROUTER:
            self.generate = self._generate_openrouter
            return OpenRouterProvider(self.api_key, self.model)
        elif provider == Providers.OPENAI:
            self.generate = self._generate_openai
            return OpenAIProvider(self.api_key, self.model)
        else:
            raise ValueError(
                f"Provider {provider} not supported for {self.__class__.__name__}"
            )

    def _generate_openai(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[
            List[ModelFormattedDictTool]
        ] = None,  # TODO should this be an optional? an empty list by default would make more sense
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will hardcode a group of default parameters for the OpenAI provider for the GPT-4o Mini model.
        """
        # Update the parameters with the ones provided in the kwargs.
        tmp = self.provider.generate(
            messages=messages,
            temperature=temperature,
            tools=tools,
            tool_choice=tool_choice,
            parallel_tool_calls=parallel_tool_calls,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=None,
            test=test,
            **kwargs,
        )
        # assert isinstance(tmp, OpenAIResponse), "tmp is not an OpenAIResponse"
        return tmp

    def _generate_openrouter(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will construct a default group of parameters for the OpenRouter provider.
        """
        tmp = self.provider.generate(
            messages=messages,
            temperature=temperature,
            tools=tools,
            tool_choice=tool_choice,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            test=test,
            **kwargs,
        )
        assert isinstance(tmp, OpenAIResponse), "tmp is not an OpenAIResponse"
        return tmp


class Gpt_4o_Mini_Latest:
    """
    The latest GPT-4o Mini model.
    """

    def __init__(self, provider: Providers, engine_id: Optional[str] = None) -> None:
        self.generate = None
        self.api_key: str = os.getenv("OPENAI_API_KEY")
        self.model: str = "gpt-4o-mini"
        self.provider = self.__getProvider(provider)
        self.engine_id = engine_id

    def __getProvider(self, provider: Providers) -> OpenAIProvider:
        """Get the provider and set the generate method."""
        if provider == Providers.OPENROUTER:
            self.generate = self.__generate_openrouter
            return OpenRouterProvider(self.api_key, self.model)
        elif provider == Providers.OPENAI:
            self.generate = self.__generate_openai
            return OpenAIProvider(self.api_key, self.model)
        else:
            raise ValueError(
                f"Provider {provider} not supported for {self.__class__.__name__}"
            )

    def __generate_openai(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will hardcode a group of default parameters for the OpenAI provider for the GPT-4o Mini model.
        """
        # Update the parameters with the ones provided in the kwargs.
        tmp = self.provider.generate(
            messages=messages,
            temperature=temperature,
            tools=tools,
            tool_choice=tool_choice,
            parallel_tool_calls=parallel_tool_calls,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            test=test,
            **kwargs,
        )
        assert isinstance(tmp, OpenAIResponse), "tmp is not an OpenAIResponse"
        return tmp

    def __generate_openrouter(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will construct a default group of parameters for the OpenRouter provider.
        """
        tmp = self.provider.generate(
            messages=messages,
            temperature=temperature,
            tools=tools,
            tool_choice=tool_choice,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            test=test,
            **kwargs,
        )
        assert isinstance(tmp, OpenAIResponse), "tmp is not an OpenAIResponse"
        return tmp


class Gpt_o3_Mini(Model):
    """
    The latest GPT-o3 Mini model.
    """

    def __init__(self, provider: Providers) -> None:
        self.generate: Optional[Any] = None
        self.api_key: str = os.getenv("OPENAI_API_KEY")
        assert self.api_key is not None, "OPENAI_API_KEY is not set"

        self.model: str = "o3-mini"  # TODO use literal
        self.provider: OpenAIProvider = self.__getProvider(provider)

    def __getProvider(self, provider: Providers) -> OpenAIProvider:
        """Get the provider and set the generate method."""
        if provider == Providers.OPENROUTER:
            self.generate = self.__generate_openrouter
            return OpenRouterProvider(self.api_key, self.model)
        elif provider == Providers.OPENAI:
            self.generate = self.__generate_openai
            return OpenAIProvider(self.api_key, self.model)
        else:
            raise ValueError(
                f"Provider {provider} not supported for {self.__class__.__name__}"
            )

    def __generate_openai(
        self,
        messages: List[Dict[str, Any]],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict[str, Any]] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will construct a default group of parameters for the OpenAI provider.
        It will update the parameters with the ones provided in the kwargs.
        """
        tmp = self.provider.generate(
            messages=messages,
            tools=tools,
            tool_choice=tool_choice,
            parallel_tool_calls=parallel_tool_calls,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            test=test,
            **kwargs,
        )
        assert isinstance(tmp, OpenAIResponse), "tmp is not an OpenAIResponse"
        return tmp

    def __generate_openrouter(
        self,
        messages: List[Dict],
        tools: Optional[List[ModelFormattedDictTool]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> OpenAIResponse:
        """
        This method will construct a default group of parameters for the OpenRouter provider.
        """
        tmp = self.provider.generate(
            messages=messages,
            tools=tools,
            tool_choice=tool_choice,
            max_completion_tokens=max_completion_tokens,
            response_format=response_format,
            reasoning_effort=reasoning_effort,
            test=test,
            **kwargs,
        )
        assert isinstance(tmp, OpenAIResponse), "tmp is not an OpenAIResponse"
        return tmp


# TODO there needs to be an interface for these models to impliment ie base class
openai_model_type = Union[Gpt41Mini, Gpt_4o_Mini_Latest, Gpt_o3_Mini]



================================================
FILE: src/llmgine/llm/models/xai_models.py
================================================
from llmgine.llm.providers import Providers
from llmgine.llm.providers.openrouter import OpenRouterProvider
from typing import List, Dict, Literal, Optional, Union, Any
import uuid
import os

from llmgine.llm.providers.response import LLMResponse
from llmgine.llm import ToolChoiceOrDictType

class Grok3Mini:

    def __init__(self, provider: Providers) -> None:
        self.id : str = str(uuid.uuid4())
        self.generate : Optional[Any] = None
        self._setProvider(provider)

    def _setProvider(self, provider: Providers) -> None:
        """Get the provider and set the generate method."""
        if provider == Providers.OPENROUTER:
            self.api_key = os.getenv("OPENROUTER_API_KEY")
            self.model = "x-ai/grok-3-mini-beta"
            self.provider = OpenRouterProvider(
                self.api_key, self.model
            )
            self.generate = self._generate_from_openrouter
        else:
            raise ValueError(
                f"Provider {provider} not supported for {self.__class__.__name__}"
            )

    def _generate_from_openrouter(
        self,
        messages: List[Dict],
        tools: Optional[List[Dict]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        temperature: float = 0.7,
        max_completion_tokens: int = 5068,
    ) -> LLMResponse:
        return self.provider.generate(
            messages=messages,
            tools=tools,
            tool_choice=tool_choice,
            temperature=temperature,
            max_completion_tokens=max_completion_tokens,
        )



================================================
FILE: src/llmgine/llm/providers/__init__.py
================================================
"""LLM Provider interfaces and implementations.

This module defines the core LLM provider protocol and manager interfaces,
as well as concrete implementations.
"""

import uuid
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Protocol

from llmgine.llm import (
    ContextType,
    ModelFormattedDictTool,
    ModelNameStr,
)
from llmgine.llm.providers.response import LLMResponse
from llmgine.llm.tools import ToolCall


class LLMProvider(Protocol):
    """Protocol defining the interface for an LLM provider."""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        context: Optional[ContextType] = None,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model: Optional[ModelNameStr] = None,
        tools: Optional[List[ModelFormattedDictTool]] = None,
    ) -> LLMResponse:
        """Generate a response from the LLM.

        Args:
            prompt: The user prompt to send to the LLM
            context: Optional conversation context/history
            system_prompt: Optional system prompt/instructions
            temperature: Optional temperature parameter
            max_tokens: Optional maximum tokens for the response
            model: Optional model name/identifier
            tools: Optional list of tools to provide to the LLM
            **kwargs: Additional provider-specific parameters

        Returns:
            LLMResponse: The response from the LLM
        """
        ...


class LLMManager(ABC):
    """Interface for managing LLM providers and models."""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        provider_id: Optional[str] = None,
        context: Optional[ContextType] = None,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model: Optional[ModelNameStr] = None,
        tools: Optional[List[ModelFormattedDictTool]] = None,
    ) -> LLMResponse:
        """Generate a response from an LLM provider.

        Args:
            prompt: The user prompt to send to the LLM
            provider_id: Optional provider identifier, uses default if not specified
            context: Optional conversation context/history
            system_prompt: Optional system prompt/instructions
            temperature: Optional temperature parameter
            max_tokens: Optional maximum tokens for the response
            model: Optional model name/identifier
            tools: Optional list of tools to provide to the LLM
            **kwargs: Additional provider-specific parameters

        Returns:
            LLMResponse: The response from the LLM
        """
        ...

    @abstractmethod
    def register_provider(self, provider_id: str, provider: LLMProvider) -> None:
        """Register an LLM provider.

        Args:
            provider_id: The identifier for the provider
            provider: The LLM provider implementation
        """
        ...

    @abstractmethod
    def get_provider(self, provider_id: Optional[str] = None) -> LLMProvider:
        """Get an LLM provider.

        Args:
            provider_id: The provider identifier, uses default if not specified

        Returns:
            LLMProvider: The requested LLM provider

        Raises:
            ValueError: If the provider is not found
        """
        ...

    @abstractmethod
    def set_default_provider(self, provider_id: str) -> None:
        """Set the default LLM provider.

        Args:
            provider_id: The provider identifier to set as default

        Raises:
            ValueError: If the provider is not found
        """
        ...


class DefaultLLMManager(LLMManager):
    """Default implementation of the LLM manager interface."""

    def __init__(self, default_provider_id: Optional[str] = None):
        """Initialize the default LLM manager.

        Args:
            default_provider_id: The default provider ID to use if not specified
        """
        self.providers: Dict[str, LLMProvider] = {}
        self.default_provider_id = default_provider_id

    async def generate(
        self,
        prompt: str,
        provider_id: Optional[str] = None,
        context: Optional[ContextType] = None,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model: Optional[ModelNameStr] = None,
        tools: Optional[List[ModelFormattedDictTool]] = None,
    ) -> LLMResponse:
        """Generate a response from an LLM provider.

        Args:
            prompt: The user prompt to send to the LLM
            provider_id: Optional provider identifier, uses default if not specified
            context: Optional conversation context/history
            system_prompt: Optional system prompt/instructions
            temperature: Optional temperature parameter
            max_tokens: Optional maximum tokens for the response
            model: Optional model name/identifier
            tools: Optional list of tools to provide to the LLM
            **kwargs: Additional provider-specific parameters

        Returns:
            LLMResponse: The response from the LLM

        Raises:
            ValueError: If the provider is not found
        """
        provider = self.get_provider(provider_id)

        return await provider.generate(
            prompt=prompt,
            context=context,
            system_prompt=system_prompt,
            temperature=temperature,
            max_tokens=max_tokens,
            model=model,
            tools=tools,
        )

    def register_provider(self, provider_id: str, provider: LLMProvider) -> None:
        """Register an LLM provider.

        Args:
            provider_id: The identifier for the provider
            provider: The LLM provider implementation
        """
        self.providers[provider_id] = provider

        # Set as default if it's the first provider or none is set
        if self.default_provider_id is None:
            self.default_provider_id = provider_id

    def get_provider(self, provider_id: Optional[str] = None) -> LLMProvider:
        """Get an LLM provider.

        Args:
            provider_id: The provider identifier, uses default if not specified

        Returns:
            LLMProvider: The requested LLM provider

        Raises:
            ValueError: If the provider is not found
        """
        # Use default provider if none specified
        if provider_id is None:
            if self.default_provider_id is None:
                raise ValueError("No default provider is set")
            provider_id = self.default_provider_id

        # Get the provider
        if provider_id not in self.providers:
            raise ValueError(f"Provider '{provider_id}' not found")

        return self.providers[provider_id]

    def set_default_provider(self, provider_id: str) -> None:
        """Set the default LLM provider.

        Args:
            provider_id: The provider identifier to set as default

        Raises:
            ValueError: If the provider is not found
        """
        if provider_id not in self.providers:
            raise ValueError(f"Provider '{provider_id}' not found")

        self.default_provider_id = provider_id


# Helper function to create a tool call
def create_tool_call(name: str, arguments: Dict[str, Any]) -> ToolCall:
    """Create a standardized tool call object.

    Args:
        name: The name of the tool to call
        arguments: The arguments to pass to the tool

    Returns:
        A ToolCall object
    """
    return ToolCall(id=str(uuid.uuid4()), name=name, arguments=str(arguments))


# Import specific provider implementations
from llmgine.llm.providers.dummy import DummyProvider
from llmgine.llm.providers.openai_provider import OpenAIProvider
from llmgine.llm.providers.providers import Providers

__all__ = [
    "DummyProvider",
    "OpenAIProvider",
    "Providers",
]



================================================
FILE: src/llmgine/llm/providers/anthropic_provider.py
================================================
"""OpenAI provider implementation."""

import uuid
from typing import Any, Dict, List, Literal, Optional, Union

from anthropic import AsyncAnthropic
from openai.types.chat import ChatCompletion

from llmgine.bootstrap import ApplicationConfig
from llmgine.bus.bus import MessageBus
from llmgine.llm.providers import LLMProvider
from llmgine.llm.providers.events import LLMCallEvent, LLMResponseEvent
from llmgine.llm.providers.providers import Providers
from llmgine.llm.providers.response import LLMResponse, ResponseTokens
from llmgine.llm.tools.toolCall import ToolCall
from llmgine.llm import ToolChoiceOrDictType

class AnthropicResponse(LLMResponse):
    def __init__(self, response: ChatCompletion) -> None:
        self.response = response

    @property
    def raw(self) -> ChatCompletion:
        return self.response

    @property
    def content(self) -> str:
        return self.response.content[0].text

    @property
    def tool_calls(self) -> List[ToolCall]:
        return [
            ToolCall(tool_call)
            for tool_call in self.response.choices[0].message.tool_calls
        ]

    @property
    def has_tool_calls(self) -> bool:
        return len(self.tool_calls) > 0

    @property
    def finish_reason(self) -> str:
        return self.response.choices[0].finish_reason

    @property
    def tokens(self) -> ResponseTokens:
        return ResponseTokens(
            prompt_tokens=self.response.usage.prompt_tokens,
            completion_tokens=self.response.usage.completion_tokens,
            total_tokens=self.response.usage.total_tokens,
        )

    @property
    def reasoning(self) -> str:
        return self.response.choices[0].message.reasoning


class AnthropicProvider(LLMProvider):
    def __init__(
        self, api_key: str, model: str, model_component_id: Optional[str] = None
    ) -> None:
        self.model = model
        self.model_component_id = model_component_id
        self.client = AsyncAnthropic(api_key=api_key)
        self.bus = MessageBus()

    async def generate(
        self,
        messages: List[Dict],
        tools: Optional[List[Dict]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict] = None,
        thinking_enabled: bool = False,
        thinking_budget: Optional[int] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> LLMResponse:
        call_id = str(uuid.uuid4())

        # construct the payload
        payload = {
            "model": self.model,
            "messages": messages,
            "max_tokens": max_completion_tokens,
        }

        # System prompt extract
        if messages[0]["role"] == "system":
            payload["system"] = messages[0]["content"]
            payload["messages"] = messages[1:]

        if temperature:
            payload["temperature"] = temperature

        if tools:
            payload["tools"] = tools

            if tool_choice:
                payload["tool_choice"] = tool_choice

            if parallel_tool_calls is not None:
                payload["parallel_tool_calls"] = parallel_tool_calls

        if response_format:
            payload["response_format"] = response_format

        if thinking_enabled:
            payload["thinking"] = {
                "type": "enabled",
                "budget": thinking_budget,
            }

        payload.update(**kwargs)
        call_event = LLMCallEvent(
            call_id=call_id,
            model_id=self.model_component_id,
            provider=Providers.ANTHROPIC,
            payload=payload,
        )
        await self.bus.publish(call_event)
        try:
            response = await self.client.messages.create(**payload)
        except Exception as e:
            await self.bus.publish(
                LLMResponseEvent(
                    call_id=call_id,
                    error=e,
                )
            )
            raise e
        await self.bus.publish(
            LLMResponseEvent(
                call_id=call_id,
                raw_response=response,
            )
        )
        if test:
            return response
        else:
            return AnthropicResponse(response)

    def stream() -> None:
        # TODO: Implement streaming
        raise NotImplementedError("Streaming is not supported for OpenAI")


async def main():
    import os

    import dotenv

    from llmgine.bootstrap import ApplicationBootstrap

    dotenv.load_dotenv(override=True)
    app = ApplicationBootstrap(ApplicationConfig(enable_console_handler=False))
    await app.bootstrap()
    provider = AnthropicProvider(
        api_key=os.getenv("ANTHROPIC_API_KEY"), model="claude-3-5-sonnet-20240620"
    )
    response = await provider.generate(
        messages=[
            {"role": "system", "content": "Respond in pirate language"},
            {"role": "user", "content": "Hello, how are you?"},
        ]
    )
    print(response.content)


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())



================================================
FILE: src/llmgine/llm/providers/dummy.py
================================================
"""Dummy LLM provider implementation for testing purposes."""

from typing import Any, Dict, List, Optional

from llmgine.llm.providers import LLMProvider
from llmgine.llm.providers.response import LLMResponse


class DummyProvider(LLMProvider):
    """A dummy provider for testing purposes."""

    async def generate(
        self,
        prompt: str,
        context: Optional[List[Dict[str, Any]]] = None,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        model: Optional[str] = None,
        **kwargs,
    ) -> LLMResponse:
        """Generate a response from the dummy LLM.

        Args:
            prompt: The user prompt to send to the LLM
            context: Optional conversation context/history
            system_prompt: Optional system prompt/instructions
            temperature: Optional temperature parameter
            max_tokens: Optional maximum tokens for the response
            model: Optional model name/identifier
            **kwargs: Additional provider-specific parameters

        Returns:
            LLMResponse: The response from the LLM
        """
        return LLMResponse(
            content=f"This is a dummy response to: {prompt}",
            role="assistant",
            model="dummy-model",
            finish_reason="stop",
            usage={"prompt_tokens": 10, "completion_tokens": 10, "total_tokens": 20},
        )



================================================
FILE: src/llmgine/llm/providers/events.py
================================================
from dataclasses import dataclass, field
from typing import Any, Dict, Optional

from llmgine.llm.providers.providers import Providers
from llmgine.messages.events import Event


@dataclass
class LLMResponseEvent(Event):
    call_id: str = ""
    raw_response: Dict[str, Any] = field(default_factory=dict)
    error: Optional[Exception] = None


@dataclass
class LLMCallEvent(Event):
    model_id: str = ""
    call_id: str = ""
    provider: Optional[Providers] = None
    payload: Dict[str, Any] = field(default_factory=dict)



================================================
FILE: src/llmgine/llm/providers/gemini.py
================================================
[Empty file]


================================================
FILE: src/llmgine/llm/providers/litellm.py
================================================
# """LiteLLM provider implementation for unified access to multiple LLM providers."""

# from typing import Any, Dict, List, Optional

# import litellm

# from llmgine.llm.providers import LLMProvider
# from llmgine.messages.events import LLMResponse


# class LiteLLMProvider(LLMProvider):
#     """Provider implementation using LiteLLM for unified access to multiple LLM APIs."""

#     def __init__(
#         self,
#         default_model: str = "gpt-3.5-turbo",
#         api_key: Optional[str] = None,
#         **config,
#     ):
#         """Initialize the LiteLLM provider.

#         Args:
#             default_model: The default model to use if none is specified
#             api_key: Optional API key for the LLM service
#             **config: Additional configuration parameters for LiteLLM
#         """
#         self.default_model = default_model
#         self.api_key = api_key
#         self.config = config

#         # Set API key if provided
#         if api_key:
#             litellm.api_key = api_key

#     async def generate(
#         self,
#         prompt: str,
#         context: Optional[List[Dict[str, Any]]] = None,
#         system_prompt: Optional[str] = None,
#         temperature: Optional[float] = None,
#         max_tokens: Optional[int] = None,
#         model: Optional[str] = None,
#         **kwargs,
#     ) -> LLMResponse:
#         """Generate a response using LiteLLM.

#         Args:
#             prompt: The user prompt to send to the LLM
#             context: Optional conversation context/history
#             system_prompt: Optional system prompt/instructions
#             temperature: Optional temperature parameter
#             max_tokens: Optional maximum tokens for the response
#             model: Optional model name/identifier
#             **kwargs: Additional provider-specific parameters

#         Returns:
#             LLMResponse: The response from the LLM
#         """
#         # Use default model if none provided
#         model_name = model or self.default_model

#         # Build messages for the conversation
#         messages = []

#         # Add system message if provided
#         if system_prompt:
#             messages.append({"role": "system", "content": system_prompt})

#         # Add context messages if provided
#         if context:
#             messages.extend(context)

#         # Add the current prompt
#         messages.append({"role": "user", "content": prompt})

#         # Set up parameters for the completion
#         completion_params = {
#             "model": model_name,
#             "messages": messages,
#         }

#         # Add optional parameters
#         if temperature is not None:
#             completion_params["temperature"] = temperature
#         if max_tokens is not None:
#             completion_params["max_tokens"] = max_tokens

#         # Add any additional parameters
#         completion_params.update(kwargs)

#         try:
#             # Call LiteLLM async completion
#             response: ModelResponse = await litellm.acompletion(**completion_params)

#             # Extract content from response
#             content = response.choices[0].message.content

#             # Get usage data
#             usage = {
#                 "prompt_tokens": response.usage.prompt_tokens,
#                 "completion_tokens": response.usage.completion_tokens,
#                 "total_tokens": response.usage.total_tokens,
#             }

#             # Return formatted response
#             return LLMResponse(
#                 content=content,
#                 role="assistant",
#                 model=model_name,
#                 finish_reason=response.choices[0].finish_reason,
#                 usage=usage,
#             )
#         except Exception as e:
#             # In a production environment, you might want more sophisticated
#             # error handling and possibly retry logic here
#             raise RuntimeError(f"LiteLLM generation failed: {str(e)}") from e



================================================
FILE: src/llmgine/llm/providers/llm_manager_events.py
================================================
from dataclasses import dataclass, field
from typing import Any, Dict

from llmgine.messages.events import Event


@dataclass
class LLMResponseEvent(Event):
    llm_manager_id: str = ""
    engine_id: str = ""
    raw_response: Dict[str, Any] = field(default_factory=dict)



================================================
FILE: src/llmgine/llm/providers/openai_provider.py
================================================
"""OpenAI provider implementation."""

import uuid
from typing import Any, Dict, List, Literal, Optional, Union

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion

from llmgine.bus.bus import MessageBus
from llmgine.llm.providers import LLMProvider
from llmgine.llm.providers.events import LLMCallEvent, LLMResponseEvent
from llmgine.llm.providers.providers import Providers
from llmgine.llm.providers.response import LLMResponse, ResponseTokens
from llmgine.llm.tools.toolCall import ToolCall
from llmgine.llm import ToolChoiceOrDictType

class OpenAIResponse(LLMResponse):
    def __init__(self, response: ChatCompletion) -> None:
        self.response = response

    @property
    def raw(self) -> ChatCompletion:
        return self.response

    @property
    def content(self) -> str:
        return self.response.choices[0].message.content

    @property
    def tool_calls(self) -> List[ToolCall]:
        return [
            ToolCall(tool_call)
            for tool_call in self.response.choices[0].message.tool_calls
        ]

    @property
    def has_tool_calls(self) -> bool:
        return len(self.tool_calls) > 0

    @property
    def finish_reason(self) -> str:
        return self.response.choices[0].finish_reason

    @property
    def tokens(self) -> ResponseTokens:
        return ResponseTokens(
            prompt_tokens=self.response.usage.prompt_tokens,
            completion_tokens=self.response.usage.completion_tokens,
            total_tokens=self.response.usage.total_tokens,
        )

    @property
    def reasoning(self) -> str:
        return self.response.choices[0].message.reasoning


class OpenAIProvider(LLMProvider):
    def __init__(
        self, api_key: str, model: str, model_component_id: Optional[str] = None
    ) -> None:
        self.model = model
        self.model_component_id = model_component_id
        self.base_url = "https://api.openai.com/v1"
        self.client = AsyncOpenAI(api_key=api_key, base_url=self.base_url)
        self.bus = MessageBus()

    async def generate(
        self,
        messages: List[Dict],
        tools: Optional[List[Dict]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        parallel_tool_calls: Optional[bool] = None,
        temperature: Optional[float] = None,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        test: bool = False,
        **kwargs: Any,
    ) -> LLMResponse:
        call_id = str(uuid.uuid4())

        # construct the payload
        payload = {
            "model": self.model,
            "messages": messages,
            "max_completion_tokens": max_completion_tokens,
        }

        if temperature:
            payload["temperature"] = temperature

        if tools:
            payload["tools"] = tools

            if tool_choice:
                payload["tool_choice"] = tool_choice

            if parallel_tool_calls is not None:
                payload["parallel_tool_calls"] = parallel_tool_calls

        if response_format:
            payload["response_format"] = response_format

        if reasoning_effort:
            payload["reasoning_effort"] = reasoning_effort

        payload.update(**kwargs)
        call_event = LLMCallEvent(
            call_id=call_id,
            model_id=self.model_component_id,
            provider=Providers.OPENAI,
            payload=payload,
        )
        await self.bus.publish(call_event)
        try:
            response = await self.client.chat.completions.create(**payload)
        except Exception as e:
            await self.bus.publish(
                LLMResponseEvent(
                    call_id=call_id,
                    error=e,
                )
            )
            raise e
        await self.bus.publish(
            LLMResponseEvent(
                call_id=call_id,
                raw_response=response,
            )
        )
        if test:
            return response
        else:
            return OpenAIResponse(response)

    def stream():
        # TODO: Implement streaming
        raise NotImplementedError("Streaming is not supported for OpenAI")



================================================
FILE: src/llmgine/llm/providers/openrouter.py
================================================
import uuid
from typing import Any, Dict, List, Literal, Optional, Union

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion
from prompt_toolkit import Application

from llmgine.bus.bus import MessageBus
from llmgine.llm.providers import LLMProvider
from llmgine.llm.providers.events import LLMCallEvent, LLMResponseEvent
from llmgine.llm.providers.providers import Providers
from llmgine.llm.providers.response import LLMResponse, ResponseTokens
from llmgine.llm.tools.toolCall import ToolCall
from llmgine.llm import ToolChoiceOrDictType

OpenRouterProviders = Literal[
    "OpenAI",
    "Anthropic",
    "Google",
    "Google AI Studio",
    "Amazon Bedrock",
    "Groq",
    "SambaNova",
    "Cohere",
    "Mistral",
    "Together",
    "Together 2",
    "Fireworks",
    "DeepInfra",
    "Lepton",
    "Novita",
    "Avian",
    "Lambda",
    "Azure",
    "Modal",
    "AnyScale",
    "Replicate",
    "Perplexity",
    "Recursal",
    "OctoAI",
    "DeepSeek",
    "Infermatic",
    "AI21",
    "Featherless",
    "Inflection",
    "xAI",
    "Cloudflare",
    "SF Compute",
    "Minimax",
    "Nineteen",
    "Liquid",
    "Stealth",
    "NCompass",
    "InferenceNet",
    "Friendli",
    "AionLabs",
    "Alibaba",
    "Nebius",
    "Chutes",
    "Kluster",
    "Crusoe",
    "Targon",
    "Ubicloud",
    "Parasail",
    "Phala",
    "Cent-ML",
    "Venice",
    "OpenInference",
    "Atoma",
    "01.AI",
    "HuggingFace",
    "Mancer",
    "Mancer 2",
    "Hyperbolic",
    "Hyperbolic 2",
    "Lynn 2",
    "Lynn",
    "Reflection",
]


class OpenRouterResponse(LLMResponse):
    def __init__(self, response: ChatCompletion) -> None:
        self.response = response

    @property
    def raw(self) -> ChatCompletion:
        return self.response

    @property
    def content(self) -> str:
        return self.response.choices[0].message.content

    @property
    def tool_calls(self) -> List[ToolCall]:
        return [
            ToolCall(tool_call)
            for tool_call in self.response.choices[0].message.tool_calls
        ]

    @property
    def has_tool_calls(self) -> bool:
        return len(self.tool_calls) > 0

    @property
    def finish_reason(self) -> str:
        return self.response.choices[0].finish_reason

    @property
    def tokens(self) -> ResponseTokens:
        return ResponseTokens(
            prompt_tokens=self.response.usage.prompt_tokens,
            completion_tokens=self.response.usage.completion_tokens,
            total_tokens=self.response.usage.total_tokens,
        )

    @property
    def reasoning(self) -> str:
        return self.response.choices[0].message.reasoning


class OpenRouterProvider(LLMProvider):
    def __init__(
        self,
        api_key: str,
        model: str,
        provider: Optional[OpenRouterProviders] = None,
        model_component_id: Optional[str] = None,
    ) -> None:
        self.model = model
        self.model_component_id = model_component_id
        self.provider = provider
        self.base_url = "https://openrouter.ai/api/v1"
        self.client = AsyncOpenAI(api_key=api_key, base_url=self.base_url)
        self.bus = MessageBus()

    async def generate(
        self,
        messages: List[Dict],
        tools: Optional[List[Dict]] = None,
        tool_choice: ToolChoiceOrDictType = "auto",
        temperature: float = 0.7,
        max_completion_tokens: int = 5068,
        response_format: Optional[Dict] = None,
        reasoning: bool = False,
        reasoning_max_tokens: Optional[int] = None,
        reasoning_effort: Optional[Literal["low", "medium", "high"]] = None,
        reasoning_include_reasoning: Optional[bool] = False,
        retry_count: int = 5,
        test: bool = False,
        **kwargs: Any,
    ) -> LLMResponse:
        call_id = str(uuid.uuid4())

        # Default payload
        payload = {
            "model": self.model,
            "messages": messages,
            "max_completion_tokens": max_completion_tokens,
        }

        # Provider specific
        if self.provider:
            payload["extra_body"] = {
                "provider": {
                    "order": [self.provider],
                    "allow_fallbacks": False,
                    "data_collection": "deny",
                }
            }

        # Temperature
        if temperature:
            payload["temperature"] = temperature

        # Tools
        if tools:
            payload["tools"] = tools

            if tool_choice:
                payload["tool_choice"] = tool_choice

        # Response format
        if response_format:
            payload["response_format"] = response_format

        # Reasoning
        if reasoning_effort:
            payload["reasoning_effort"] = reasoning_effort

        # Reasoning
        if reasoning:
            payload["extra_body"]["reasoning"] = {}
            if reasoning_max_tokens:
                payload["extra_body"]["reasoning"]["max_tokens"] = reasoning_max_tokens
            if reasoning_effort:
                payload["extra_body"]["reasoning"]["effort"] = reasoning_effort
            if not reasoning_include_reasoning:
                payload["extra_body"]["reasoning"]["exclude"] = True

        # Update payload with additional kwargs
        payload.update(**kwargs)  # type: ignore

        # Call event
        call_event = LLMCallEvent(
            call_id=call_id,
            model_id=self.model_component_id,
            provider=Providers.OPENROUTER,
            payload=payload,
        )
        await self.bus.publish(call_event)
        for _ in range(retry_count):
            try:
                response = await self.client.chat.completions.create(**payload)
                await self.bus.publish(
                    LLMResponseEvent(
                        call_id=call_id,
                        raw_response=response,
                    )
                )
                break
            except Exception as e:
                await self.bus.publish(
                    LLMResponseEvent(
                        call_id=call_id,
                        error=e,
                    )
                )
        if test:
            # Return raw response
            return response
        else:
            # Return wrapped response
            return OpenRouterResponse(response)

    def stream() -> None:
        # TODO: Implement streaming
        raise NotImplementedError("Streaming is not supported for OpenRouter")


async def main():
    from llmgine.bootstrap import ApplicationBootstrap, ApplicationConfig
    import os
    from llmgine.llm.tools import ToolManager

    def get_weather(city: str) -> str:
        """
        Get the weather in a city.

        Args:
            city: The city to get the weather for.

        Returns:
            str: The weather in the city.
        """
        return f"The weather in {city} is sunny with a chance to rain meatballs."

    app = ApplicationBootstrap(ApplicationConfig(enable_console_handler=False))
    await app.bootstrap()
    tool_manager = ToolManager(session_id="test", engine_id="test")
    await tool_manager.register_tool(get_weather)
    provider = OpenRouterProvider(
        api_key=os.getenv("OPENROUTER_API_KEY"),
        model="deepseek/deepseek-chat-v3-0324",
        provider="Fireworks",
    )
    tools = await tool_manager.get_tools()
    response = await provider.generate(
        messages=[{"role": "user", "content": "Whats the weather in Tokyo?"}],
        tools=tools,
        test=True,
    )
    print(response)


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())



================================================
FILE: src/llmgine/llm/providers/providers.py
================================================
from enum import Enum
from typing import Any
from llmgine.llm.providers.response import LLMResponse


class Providers(Enum):
    OPENAI = "openai"
    LITELLM = "litellm"
    OPENROUTER = "openrouter"
    GEMINI = "gemini"
    VERTEX_AI = "vertex_ai"
    ANTHROPIC = "anthropic"
    COHERE = "cohere"


class Provider:
    def generate(self, **kwargs: Any) -> LLMResponse:
        raise NotImplementedError



================================================
FILE: src/llmgine/llm/providers/response.py
================================================
# parsing a response for a unified interface


import logging
from dataclasses import dataclass
from typing import Any, Dict, List

from llmgine.llm.tools.toolCall import ToolCall

# Set up logging
logger = logging.getLogger(__name__)


@dataclass
class ResponseTokens:
    # TODO: better structure, cost calculation, etc
    prompt_tokens: int
    reasoning_tokens: int
    completion_tokens: int
    total_tokens: int


@dataclass
class ResponseMetrics:
    # TODO: better structure, cost calculation, etc
    tokens: ResponseTokens
    cost: float
    ttf: float
    tps: float


# Base class for LLM responses
class LLMResponse:
    def __init__(self, raw_response: Any):
        self.raw = raw_response

    @property
    def content(self) -> str:
        raise NotImplementedError

    @property
    def tool_calls(self) -> List[ToolCall]:
        raise NotImplementedError

    @property
    def has_tool_calls(self) -> bool:
        raise NotImplementedError

    @property
    def finish_reason(self) -> str:
        raise NotImplementedError

    @property
    def tokens(self) -> ResponseTokens:
        raise NotImplementedError

    @property
    def metrics(self) -> Dict[str, Any]:
        raise NotImplementedError

    @property
    def model(self) -> Dict[str, Any]:
        raise NotImplementedError

    @property
    def reasoning(self) -> str:
        raise NotImplementedError



================================================
FILE: src/llmgine/llm/tools/README.md
================================================
# Read this to understand what happend within TOOLS


================================================
FILE: src/llmgine/llm/tools/__init__.py
================================================
"""Tools for LLMs.

This module provides tools that can be called by language models.
"""

# Re-export tool call events from messages
from llmgine.llm import AsyncToolFunction, ToolFunction
from llmgine.llm.tools.tool import Parameter, Tool
from llmgine.llm.tools.tool_manager import ToolManager
from llmgine.llm.tools.toolCall import ToolCall

__all__ = [
    "ToolCall",
    "Tool",
    "ToolManager",
    "ToolFunction",
    "AsyncToolFunction",
    "Parameter",
]



================================================
FILE: src/llmgine/llm/tools/tool.py
================================================
from dataclasses import dataclass
from typing import Any, Dict, List

from llmgine.llm import AsyncOrSyncToolFunction


@dataclass
class Parameter:
    """A parameter for a tool.

    Attributes:
        name: The name of the parameter
        description: A description of the parameter
        type: The type of the parameter
        required: Whether the parameter is required
    """

    name: str
    description: str
    type: str
    required: bool = False

    def __init__(self, name: str, description: str, type: str, required: bool = False):
        self.name = name
        self.description = description or ""
        self.type = type
        self.required = required

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "type": self.type,
            "required": self.required,
        }


@dataclass
class Tool:
    """Contains all information about a tool.

    Attributes:
        name: The name of the tool
        description: A description of what the tool does
        parameters: JSON schema for the tool parameters
        function: The function to call when the tool is invoked
        is_async: Whether the function is asynchronous
    """

    name: str
    description: str
    parameters: List[Parameter]
    function: AsyncOrSyncToolFunction
    is_async: bool = False

    def to_dict(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "parameters": [param.to_dict() for param in self.parameters],
            "is_async": self.is_async,
        }



================================================
FILE: src/llmgine/llm/tools/tool_events.py
================================================
"""This module defines the different events that can be
emitted by the ToolManager.
"""

from dataclasses import dataclass, field
from typing import Dict, Any, List

from llmgine.messages.events import Event
#from llmgine.llm.tools.types import SessionID

@dataclass
class ToolManagerEvent(Event):
    
    # TODO idk about this
    tool_manager_id: str = field(default_factory=str)
    engine_id: str = field(default_factory=str)
    session_id: str = field(default_factory=str)


@dataclass
class ToolRegisterEvent(ToolManagerEvent):
    tool_info: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ToolCompiledEvent(ToolManagerEvent):
    tool_compiled_list: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class ToolExecuteResultEvent(ToolManagerEvent):
    execution_succeed: bool = False
    tool_info: Dict[str, Any] = field(default_factory=dict)
    tool_args: Dict[str, Any] = field(default_factory=dict)
    tool_result: str = ""



================================================
FILE: src/llmgine/llm/tools/tool_manager.py
================================================
"""Tool management and execution for LLMs.

This module provides a way to register, describe, and execute tools
that can be called by language models.
"""

import json
import uuid
from typing import Any, List, Optional

from llmgine.bus.bus import MessageBus
from llmgine.llm import AsyncOrSyncToolFunction, ModelFormattedDictTool
from llmgine.llm.tools.tool import Tool
from llmgine.llm.tools.tool_events import (
    ToolCompiledEvent,
    ToolExecuteResultEvent,
    ToolRegisterEvent,
)
from llmgine.llm.tools.tool_parser import (
    ClaudeToolParser,
    DeepSeekToolParser,
    OpenAIToolParser,
    ToolParser,
)
from llmgine.llm.tools.tool_register import ToolRegister
from llmgine.llm.tools.toolCall import ToolCall
from llmgine.llm import SessionID

class ToolManager:
    """Manages tool registration and execution."""

    def __init__(
        self, engine_id: str, session_id: SessionID, llm_model_name: Optional[str] = None
    ):
        """Initialize the tool manager."""
        self.tool_manager_id = str(uuid.uuid4())
        self.engine_id: str = engine_id  # TODO make type
        self.session_id: SessionID = session_id
        self.message_bus: MessageBus = MessageBus()
        self.tools: dict[str, Tool] = {}
        self.__tool_parser: ToolParser = self._get_parser(llm_model_name)
        self.__tool_register: ToolRegister = ToolRegister()

    async def register_tool(self, tool_function: AsyncOrSyncToolFunction) -> None:
        """Register a tool, tool manager will publish the tool
            registration event and hand it to the tool register.

        Args:
            tool: The tool to register
        """

        name: str
        tool: Tool
        name, tool = self.__tool_register.register_tool(tool_function)

        self.tools[name] = tool

        # Publish the tool registration event
        await self.message_bus.publish(
            ToolRegisterEvent(
                tool_manager_id=self.tool_manager_id,
                session_id=self.session_id,
                engine_id=self.engine_id,
                tool_info=tool.to_dict(),
            )
        )

    async def register_tools(self, platform_list: List[str]):
        """Register tools for a specific platform. Completely independent from register_tool.

        Args:
            platform_list: A list of platform names
        """

        # Register tools for each platform
        for name, tool in self.__tool_register.register_tools(platform_list).items():
            self.tools[name] = tool

            # Publish the tool registration event
            await self.message_bus.publish(
                ToolRegisterEvent(
                    tool_manager_id=self.tool_manager_id,
                    session_id=self.session_id,
                    engine_id=self.engine_id,
                    tool_info=tool.to_dict(),
                )
            )

    async def get_tools(self) -> list[ModelFormattedDictTool]:
        """Get all registered tools from the tool register.

        Returns:
            A list of tools in the registered model's format
        """
        # Collect all tools from the tool register
        tools = list(self.tools.values())

        # Publish the tool compilation event
        await self.message_bus.publish(
            ToolCompiledEvent(
                tool_manager_id=self.tool_manager_id,
                session_id=self.session_id,
                engine_id=self.engine_id,
                tool_compiled_list=[tool.to_dict() for tool in tools],
            )
        )

        ret: list[ModelFormattedDictTool] = [
            self.__tool_parser.parse_tool(tool) for tool in tools
        ]

        return ret

    async def execute_tool_call(self, tool_call: ToolCall) -> Optional[Any]:
        """Execute a tool from a ToolCall object.

        Args:
            tool_call: The tool call to execute

        Returns:
            The result of the tool execution

        Raises:
            ValueError: If the tool is not found
        """
        tool_name : str = tool_call.name

        try:
              # Parse arguments
            arguments : dict[str, Any] = json.loads(tool_call.arguments)
            assert isinstance(arguments, dict)
            return await self.__execute_tool(tool_name, arguments)
        except json.JSONDecodeError as e:
            error_msg : str = f"Invalid JSON arguments for tool {tool_name}: {e}"
            raise ValueError(error_msg) from e

    async def __execute_tool(self, tool_name: str, arguments: dict[str, Any]) -> Any:
        """Execute a tool with the given arguments.

        Args:
            tool_name: The name of the tool to execute
            arguments: The arguments to pass to the tool

        Returns:
            The result of the tool execution

        Raises:
            ValueError: If the tool is not found
        """
        if tool_name not in self.tools:
            error_msg : str = f"Tool not found: {tool_name}"
            raise ValueError(error_msg)

        tool : Tool = self.tools[tool_name]

        try:
            # Call the tool function with the provided arguments
            if tool.is_async:
                result = await tool.function(**arguments)
            else:
                result = tool.function(**arguments)

            # Publish the tool execution event
            await self.message_bus.publish(
                ToolExecuteResultEvent(
                    tool_manager_id=self.tool_manager_id,
                    session_id=self.session_id,
                    engine_id=self.engine_id,
                    execution_succeed=True,
                    tool_info=tool.to_dict(),
                    tool_args=arguments,
                    tool_result=str(result),
                )
            )
            return result
        except Exception as e:
            # Publish the tool execution event
            await self.message_bus.publish(
                ToolExecuteResultEvent(
                    tool_manager_id=self.tool_manager_id,
                    session_id=self.session_id,
                    engine_id=self.engine_id,
                    execution_succeed=False,
                    tool_info=tool.to_dict(),
                    tool_args=arguments,
                    tool_result=str(e),
                )
            )

            return f"ERROR: {e!s}"

    def _get_parser(self, llm_model_name: Optional[str] = None) -> ToolParser:
        """Get the appropriate tool parser based on the LLM model name."""
        if llm_model_name == "openai":
            tool_parser: ToolParser = OpenAIToolParser()
        elif llm_model_name == "claude":
            tool_parser = ClaudeToolParser()
        elif llm_model_name == "deepseek":
            tool_parser = DeepSeekToolParser()
        else:
            tool_parser = OpenAIToolParser()
        return tool_parser



================================================
FILE: src/llmgine/llm/tools/tool_parser.py
================================================
"""Tool parsing for LLMs.

This module provides a way to parse tools into a format that can be used by
any LLM.
"""

from abc import abstractmethod
from typing import Any

from llmgine.llm import ModelFormattedDictTool
from llmgine.llm.tools.tool import Tool


class ToolParser:
    @abstractmethod
    def parse_tool(self, tool: Tool) -> ModelFormattedDictTool:
        """Parse a tool into a format that can be used by any LLM."""
        pass


def create_required_and_properties(tool: Tool) -> tuple[list[str], dict[str, Any]]:
    required: list[str] = []
    properties: dict[str, Any] = {}
    for param in tool.parameters:
        properties[param.name] = {
            "type": param.type,
            "description": param.description,
        }
        if param.required:
            required.append(param.name)

    return required, properties


class OpenAIToolParser(ToolParser):
    """A parser for tools that can be used by OpenAI."""

    def parse_tool(self, tool: Tool) -> ModelFormattedDictTool:
        """Parse a tool into a format that can be used by OpenAI.

        Args:
            tool: The tool to be parsed.

        Returns:
            A dictionary containing the tool's name, description, and parameters.
        """

        # parameters that are required
        required, properties = create_required_and_properties(tool)

        return ModelFormattedDictTool({
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": required,
                },
            },
        })


class ClaudeToolParser(ToolParser):
    """A parser for tools that can be used by Claude."""

    def parse_tool(self, tool: Tool) -> ModelFormattedDictTool:
        """Parse a tool into a format that can be used by Claude.

        Args:
            tool: The tool to be parsed.

        Returns:
            A dictionary containing the tool's name, description, and parameters.
        """

        # parameters that are required
        required, properties = create_required_and_properties(tool)

        return ModelFormattedDictTool({
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "input_schema": {
                    "type": "object",
                    "properties": properties,
                    "required": required,
                },
            },
        })


class DeepSeekToolParser(ToolParser):
    """A parser for tools that can be used by DeepSeek."""

    def parse_tool(self, tool: Tool) -> ModelFormattedDictTool:
        """Parse a tool into a format that can be used by DeepSeek.

        Args:
            tool: The tool to be parsed.

        Returns:
            A dictionary containing the tool's name, description, and parameters.
        """

        # parameters that are required
        required, properties = create_required_and_properties(tool)

        return ModelFormattedDictTool({
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": {
                    "type": "object",
                    "properties": properties,
                    "required": required,
                },
            },
        })



================================================
FILE: src/llmgine/llm/tools/tool_register.py
================================================
import asyncio
import importlib
import inspect
import logging
import os
import re
from typing import Any, Dict, List, Tuple, Type

from llmgine.llm import AsyncOrSyncToolFunction
from llmgine.llm.tools.tool import Parameter, Tool

logger = logging.getLogger(__name__)


class ToolRegister:
    def register_tool(self, function: AsyncOrSyncToolFunction) -> Tuple[str, Tool]:
        """Register a function as a tool.

        Args:
            function: The function to register

        Raises:
            ValueError: If the function has no description
        """

        # Get the name, description, parameters, and async status of the function
        name = function.__name__
        description = self._get_function_description(function)
        parameters = self._get_function_parameters(function)
        is_async = asyncio.iscoroutinefunction(function)

        tool: Tool = Tool(
            name=name,
            description=description,
            parameters=parameters,
            function=function,
            is_async=is_async,
        )

        return name, tool  # TODO can't we just return the tool?

    def register_tools(self, platform_list: List[str]) -> Dict[str, Tool]:
        """Register all relevant tools for a list of platforms. Completely independent from register_tool.

        Args:
            platform_list: A list of platform names
        """

        tools: Dict[str, Tool] = {}
        for platform in platform_list:
            for function in self._get_functions_for_platform(platform):
                name, tool = self.register_tool(function)
                tools[name] = tool

        return tools

    def _get_functions_for_platform(self, platform: str) -> List[AsyncOrSyncToolFunction]:
        """Get all functions for a specific platform.
        The functions for 'platform' are stored in the folder 'platform_tools' with the file name '{platform.lower()}_tools.py',
        and in a variable called '{platform.upper()}_TOOLS'.

        Args:
            platform: The platform to get functions for

        Returns:
            List of tool functions for the platform
        """
        functions: List[AsyncOrSyncToolFunction] = []
        platform_tools_dir = os.path.join(os.path.dirname(__file__), "platform_tools")

        # Check directory for platform-specific files
        if os.path.exists(platform_tools_dir):
            for filename in os.listdir(platform_tools_dir):
                if f"{platform.lower()}_tools" in filename.lower() and filename.endswith(
                    ".py"
                ):
                    module_name = f"llmgine.llm.tools.platform_tools.{filename[:-3]}"
                    try:
                        module = importlib.import_module(module_name)
                        if hasattr(module, f"{platform.upper()}_TOOLS"):
                            functions.extend(getattr(module, f"{platform.upper()}_TOOLS"))
                        else:
                            logger.warning(
                                f"No tools found for {platform} in {module_name}"
                            )
                    except ImportError as e:
                        logger.error(
                            f"When registering tools for {platform}, failed to import {module_name}: {e}"
                        )

        return functions

    def _get_function_description(self, function: AsyncOrSyncToolFunction) -> str:
        """Get the description of a function.

        Args:
            function: The function to get the description of

        Returns:
            The description of the function

        Raises:
            ValueError: If the function has no description
        """
        function_desc_pattern = r"^\s*(.+?)(?=\s*Args:|$)"
        desc_doc = re.search(function_desc_pattern, function.__doc__ or "", re.MULTILINE)

        if desc_doc:
            description = desc_doc.group(1).strip()
            description = " ".join(line.strip() for line in description.split("\n"))
        else:
            raise ValueError(
                f"Function '{function.__name__}' has no description provided"
            )

        return description

    def _get_function_parameters(
        self, function: AsyncOrSyncToolFunction
    ) -> List[Parameter]:
        """Get the parameters of a function.

        Args:
            function: The function to get the parameters of

        Returns:
            A list of function parameters, each with a name, type, and description

        Raises:
            ValueError: If the function has no parameters
        """

        param_desc: str

        # Extract parameters from function signature
        sig = inspect.signature(function)
        parameters: List[Parameter] = []
        param_dict: Dict[str, str] = {}

        # Find the Args section
        args_match = re.search(
            r"Args:(.*?)(?:Returns:|Raises:|$)", function.__doc__ or "", re.DOTALL
        )
        if args_match:
            args_section = args_match.group(1).strip()

            # Pattern to match parameter documentation
            # Matches both single-line and multi-line parameter descriptions
            param_pattern = r"(\w+):\s*((?:(?!\w+:).+?\n?)+)"

            # Find all parameters in the Args section
            for match in re.finditer(param_pattern, args_section, re.MULTILINE):
                param_name = match.group(1)
                param_desc = match.group(2).strip()

                param_dict[param_name] = param_desc

        for param_name, param in sig.parameters.items():
            param_type = "string"
            param_required = False
            param_desc = f"Parameter: {param_name}"

            if param.annotation != inspect.Parameter.empty:
                # Convert type annotation to JSON schema type
                param_type = self._annotation_to_json_type(param.annotation)

            # Add to required list if no default value
            if param.default is inspect.Parameter.empty:
                param_required = True

            # If the parameter has a description in the Args section, use it
            if param_name in param_dict:
                param_desc = param_dict[param_name]

            else:
                raise ValueError(
                    f"Parameter '{param_name}' has no description in the Args section"
                )

            parameters.append(
                Parameter(
                    name=param_name,
                    description=param_desc,
                    type=param_type,
                    required=param_required,
                )
            )

        return parameters

    def _annotation_to_json_type(self, annotation: Type[Any]) -> str:
        """Convert a Python type annotation to a JSON schema type.

        Args:
            annotation: The type annotation to convert

        Returns:
            A JSON schema type string
        """
        # Simple mapping of Python types to JSON schema types
        if annotation is str:
            return "string"
        elif annotation is int:
            return "integer"
        elif annotation is float:
            return "number"
        elif annotation is bool:
            return "boolean"
        elif annotation is list or annotation is List:
            return "array"
        elif annotation is dict or annotation is Dict:
            return "object"
        else:
            # Default to string for complex types
            return "string"



================================================
FILE: src/llmgine/llm/tools/toolCall.py
================================================
from dataclasses import dataclass
from typing import Any, Dict


@dataclass
class ToolCall:
    """Represents a tool call from an LLM."""

    id: str
    type: str = "function"
    name: str = ""
    arguments: str = "{}"

    def to_dict(self) -> Dict[str, Any]:
        """Convert tool call to dictionary format."""
        return {
            "id": self.id,
            "type": self.type,
            "function": {"name": self.name, "arguments": self.arguments},
        }



================================================
FILE: src/llmgine/llm/tools/mcp/mcp_tool_manager.py
================================================
[Empty file]


================================================
FILE: src/llmgine/messages/__init__.py
================================================
"""Message types for the LLMgine system."""

from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event

__all__ = ["Command", "CommandResult", "Event"]



================================================
FILE: src/llmgine/messages/commands.py
================================================
"""Commands used in the system.

This module defines commands that can be sent on the message bus.
Commands represent actions to be performed by the system.
"""

import inspect
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from types import FrameType
from typing import Any, Dict, Optional

from llmgine.llm import SessionID


@dataclass
class Command:
    """Base class for all commands in the system.

    Commands represent actions to be performed by the system.
    Each command should be handled by exactly one handler.
    """

    command_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    metadata: Dict[str, Any] = field(default_factory=dict)
    session_id: Optional[SessionID] = None

    def __post_init__(self) -> None:
        if self.session_id is None:  # TODO  can't this be removed? and use default arg
            self.session_id = SessionID("ROOT")


@dataclass
class CommandResult:
    """Result of a command execution."""

    success: bool
    command_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    result: Optional[Any] = None
    error: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    session_id: Optional[SessionID] = None

    def __post_init__(self) -> None:
        # Add metadata about where this command was handled

        tmp: Optional[Any] = inspect.currentframe()
        assert tmp is not None
        frame: FrameType = tmp.f_back

        if frame:
            module = frame.f_globals.get("__name__", "unknown")
            function = frame.f_code.co_name
            line = frame.f_lineno
            self.metadata["finished_in"] = f"{module}.{function}:{line}"
        else:
            self.metadata["finished_in"] = "unknown"



================================================
FILE: src/llmgine/messages/events.py
================================================
"""Events used in the system.

This module defines events that can be published on the message bus.
Events represent things that have happened in the system.
"""

import inspect
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from types import FrameType
from typing import Any, Dict, Optional, override

from llmgine.llm import SessionID
from llmgine.messages.commands import Command, CommandResult


@dataclass
class Event:
    """Base class for all events in the system.

    Events represent things that have happened in the system.
    Multiple handlers can process each event.
    """

    event_id: str = field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    metadata: Dict[str, Any] = field(default_factory=dict)
    session_id: Optional[SessionID] = None

    def __post_init__(self) -> None:
        # Set the session id to GLOBAL if it is not set
        if self.session_id is None:
            self.session_id = SessionID("ROOT")

        # Add metadata about where this event was created
        tmp: Optional[Any] = inspect.currentframe()
        assert tmp is not None
        frame: FrameType = tmp.f_back
        if frame:
            module = frame.f_globals.get("__name__", "unknown")
            function = frame.f_code.co_name
            line = frame.f_lineno
            self.metadata["emitted_from"] = f"{module}.{function}:{line}"
        else:
            self.metadata["emitted_from"] = "unknown"

    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the event to a dictionary.
        """
        return {
            "event_id": self.event_id,
            "timestamp": self.timestamp,
            "metadata": self.metadata,
            "session_id": self.session_id,
        }
    
    @classmethod
    def from_dict(cls, event_dict: Dict[str, Any]) -> "Event":
        """
        Create an Event from a dictionary.
        """
        return cls(**event_dict)


@dataclass
class EventHandlerFailedEvent(Event):
    """Event emitted when an event handler fails."""

    event: Optional[Event] = None
    handler: Optional[str] = None
    exception: Optional[Exception] = None


@dataclass
class CommandStartedEvent(Event):
    """Event emitted when a command is started."""

    command: Optional[Command] = None


@dataclass
class CommandResultEvent(Event):
    """Event emitted when a command result is created."""

    command_result: Optional[CommandResult] = None

@dataclass
class ScheduledEvent(Event):
    """Base class for all scheduled events.
    
    Scheduled events are placed in the message bus and are processed
    at a specific time.
    Scheduled time must be provided. 
    If not, the event will be treated as a regular event.
    """
    scheduled_time: datetime = field(default_factory=lambda: datetime.now())

    @override
    def to_dict(self) -> Dict[str, Any]:
        """
        Convert the event to a dictionary.
        """
        return {
            "event_id": self.event_id,
            "timestamp": self.timestamp,
            "metadata": self.metadata,
            "session_id": self.session_id,
            "scheduled_time": self.scheduled_time.isoformat()
        }
    
    @classmethod
    def from_dict(cls, event_dict: Dict[str, Any]) -> "ScheduledEvent":
        """
        Create a ScheduledEvent from a dictionary.
        """
        if "scheduled_time" in event_dict:
            event_dict["scheduled_time"] = datetime.fromisoformat(event_dict["scheduled_time"])
        return cls(**event_dict)


================================================
FILE: src/llmgine/observability/__init__.py
================================================
[Empty file]


================================================
FILE: src/llmgine/observability/events.py
================================================
"""Event types specific to the observability system.

This module contains event types used primarily for observability purposes,
such as logging, tracing, and metrics.
"""

import uuid
from dataclasses import dataclass, field
from datetime import datetime
from enum import Enum
from typing import Any, Dict, Optional

from llmgine.messages.events import Event


class LogLevel(Enum):
    """Standard log levels for observability system."""

    DEBUG = "debug"
    INFO = "info"
    WARNING = "warning"
    ERROR = "error"
    CRITICAL = "critical"


@dataclass
class ObservabilityBaseEvent(Event):
    """Base class for all observability-related events.

    This is the parent class for all events that are specifically for
    observability purposes rather than domain logic. Handlers can register
    for this type to receive all observability events.
    """

    level: LogLevel = LogLevel.INFO



================================================
FILE: src/llmgine/observability/handlers/__init__.py
================================================
"""Observability event handlers."""

from .base import ObservabilityEventHandler
from .console import ConsoleEventHandler
from .file import FileEventHandler

__all__ = [
    "ObservabilityEventHandler",
    "ConsoleEventHandler",
    "FileEventHandler",
]



================================================
FILE: src/llmgine/observability/handlers/base.py
================================================
"""Base class for event observability handlers."""

from abc import ABC, abstractmethod
from typing import Any, Dict

from llmgine.messages.events import Event


class ObservabilityEventHandler(ABC):
    """Base class for handlers that process events from the MessageBus for observability purposes."""

    def __init__(self, **kwargs: Any) -> None:
        # Allow for configuration parameters
        pass

    @abstractmethod
    async def handle(self, event: Event) -> None:
        """Process an incoming event.

        Args:
            event: The event received from the MessageBus.
        """
        pass

    def event_to_dict(self, event: Any) -> Dict[str, Any]:
        """Convert an event to a dictionary representation for logging.

        Args:
            event: The event to convert

        Returns:
            Dictionary representation of the event
        """
        # Use to_dict method if available
        if hasattr(event, "to_dict") and callable(event.to_dict):
            try:
                return event.to_dict()
            except Exception:
                pass

        # Try dataclasses.asdict
        try:
            from dataclasses import asdict

            return asdict(event)
        except (TypeError, ImportError):
            pass

        # Use __dict__
        if hasattr(event, "__dict__"):
            return {k: v for k, v in event.__dict__.items() if not k.startswith("_")}

        # Fallback
        return {"event_repr": repr(event)}

    def __repr__(self) -> str:
        """Get string representation."""
        return f"{self.__class__.__name__}()"



================================================
FILE: src/llmgine/observability/handlers/console.py
================================================
"""Console handler for printing event information."""

import logging
from typing import Any

from llmgine.messages.events import Event
from llmgine.observability.handlers.base import ObservabilityEventHandler

logger = logging.getLogger(__name__)  # Use standard logger


class ConsoleEventHandler(ObservabilityEventHandler):
    """Prints a summary of events to the console using standard logging."""

    def __init__(self, **kwargs: Any) -> None:
        super().__init__(**kwargs)
        # Could add level filtering here if needed

    async def handle(self, event: Event) -> None:
        """Process the event and print relevant information to the console logger."""

        event_type = type(event).__name__
        event_dict = self.event_to_dict(event)

        # Default representation for standard events
        log_level = logging.INFO
        message = f"[EVENT] {event_type}, ID={event.id}"

        try:
            # Add session_id to message if available
            if hasattr(event, "session_id") and event.session_id:
                message += f", Session={event.session_id}"

            # Add any other relevant attributes
            if hasattr(event, "metadata") and event.metadata:
                # Extract a few key metadata items to display
                meta_display = []
                for key in ["source", "command_type", "event_type"]:
                    if key in event.metadata:
                        meta_display.append(f"{key}={event.metadata[key]}")

                if meta_display:
                    message += f" [{', '.join(meta_display)}]"

        except Exception as e:
            logger.error(
                f"Error formatting event in ConsoleEventHandler: {e}", exc_info=True
            )
            # Fall back to default message

        # Log the formatted message using the standard logger
        logger.log(log_level, message)



================================================
FILE: src/llmgine/observability/handlers/file.py
================================================
"""File handler for logging events to JSONL."""

import asyncio
from datetime import datetime
import json
import logging
import os
from pathlib import Path
import time
from typing import Any, Dict, Optional
from enum import Enum
from dataclasses import asdict

from llmgine.messages.events import Event
from llmgine.observability.handlers.base import ObservabilityEventHandler

logger = logging.getLogger(__name__)


class FileEventHandler(ObservabilityEventHandler):
    """Logs all received events to a JSONL file."""

    def __init__(
        self, log_dir: str = "logs", filename: Optional[str] = None, **kwargs: Any
    ):
        """Initialize the JSON file handler.

        Args:
            log_dir: Directory to write log files.
            filename: Optional specific filename (if None, uses timestamp).
        """
        super().__init__(**kwargs)
        self.log_dir = Path(log_dir)
        os.makedirs(self.log_dir, exist_ok=True)

        if filename:
            self.log_file = self.log_dir / filename
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.log_file = self.log_dir / f"events_{timestamp}.jsonl"

        self._file_lock = asyncio.Lock()
        logger.info(f"FileEventHandler initialized. Logging events to: {self.log_file}")

    async def handle(self, event: Event) -> None:
        """Handle the event by writing its data directly to the log file."""
        try:
            # Convert the event to dictionary for serialization
            log_data = self._event_to_dict(event)

            # Add event metadata
            log_data["event_type"] = type(event).__name__

            # Make sure parent directory exists
            os.makedirs(os.path.dirname(self.log_file), exist_ok=True)

            async with self._file_lock:
                with open(self.log_file, "a") as f:
                    f.write(json.dumps(log_data, default=str, indent=4) + "\n")
        except Exception as e:
            logger.error(f"Error writing event data to file: {e}", exc_info=True)

    def _event_to_dict(self, event: Any) -> Dict[str, Any]:
        """Convert an event (dataclass or object) to a dictionary for serialization.
        Handles nested objects, dataclasses, and Enums.
        """
        # if hasattr(event, "to_dict") and callable(event.to_dict):
        #     try:
        #         return event.to_dict()
        #     except Exception:
        #         logger.warning(f"Error calling to_dict on {type(event)}", exc_info=True)
        #         # Fall through

        try:
            # Use dataclasses.asdict with a factory to handle nested conversion
            return asdict(
                event, dict_factory=lambda x: {k: self._convert_value(v) for k, v in x}
            )
        except TypeError:
            pass  # Not a dataclass

        if hasattr(event, "__dict__"):
            return {k: self._convert_value(v) for k, v in event.__dict__.items()}

        logger.warning(
            f"Could not serialize event of type {type(event)} to dict, using repr()."
        )
        return {"event_repr": repr(event)}

    def _convert_value(self, value: Any) -> Any:
        """Helper for _event_to_dict to handle nested structures and special types."""
        if isinstance(value, Enum):
            return value.value
        elif isinstance(value, (str, int, float, bool, type(None))):
            return value
        elif isinstance(value, dict):
            return {k: self._convert_value(v) for k, v in value.items()}
        elif isinstance(value, (list, tuple)):
            return [self._convert_value(item) for item in value]
        elif hasattr(value, "__dataclass_fields__"):
            # Only handle dataclasses to avoid recursive conversion loops
            try:
                return self._event_to_dict(value)
            except Exception:
                return str(value)
        else:
            # Fallback for other objects: use string representation to prevent infinite recursion
            return str(value)



================================================
FILE: src/llmgine/prompts/__init__.py
================================================
[Empty file]


================================================
FILE: src/llmgine/prompts/prompts.py
================================================
from dataclasses import dataclass
from pathlib import Path
from typing import Any

# Helper dictionary for safe formatting
class SafeFormatterDict(dict[str, Any]):
    def __missing__(self, key: str) -> str:
        return f"{{{key}}}"

@dataclass
class Prompt:
    """Represents a prompt template that can be formatted."""
    template: str

    def format(self, **kwargs: Any) -> str:
        """
        Formats the prompt template using the provided keyword arguments.
        If a key in the template is not found in kwargs, it leaves the placeholder unchanged.

        Args:
            **kwargs: The variables to substitute into the template.

        Returns:
            The formatted prompt string.
        """
        return self.template.format_map(SafeFormatterDict(**kwargs))

def get_prompt(file_path: str | Path) -> Prompt:
    """
    Reads a markdown prompt template from a file and returns a Prompt object.

    Args:
        file_path: The path to the markdown file containing the prompt template.

    Returns:
        A Prompt object initialized with the file content.

    Raises:
        FileNotFoundError: If the file does not exist.
        IOError: If there is an error reading the file.
        ValueError: If the file extension is not .md
    """
    try:
        path = Path(file_path)
        if path.suffix.lower() != '.md':
            raise ValueError(f"Prompt file must be a markdown file (.md), got {path.suffix}")
        content = path.read_text(encoding='utf-8')
        return Prompt(template=content)
    except FileNotFoundError:
        print(f"Error: Prompt file not found at {file_path}")
        raise
    except IOError as e:
        print(f"Error reading prompt file at {file_path}: {e}")
        raise

# New function to dump prompt to file
def dump_prompt(prompt: Prompt, file_path: str | Path) -> None:
    """
    Writes the prompt template content to a markdown file.

    Args:
        prompt: The Prompt object containing the template.
        file_path: The path to the markdown file where the template should be saved.

    Raises:
        IOError: If there is an error writing the file.
        OSError: If intermediate directories cannot be created.
        ValueError: If the file extension is not .md
    """
    try:
        path = Path(file_path)
        if path.suffix.lower() != '.md':
            raise ValueError(f"Prompt file must be a markdown file (.md), got {path.suffix}")
        # Ensure the directory exists
        path.parent.mkdir(parents=True, exist_ok=True)
        path.write_text(prompt.template, encoding='utf-8')
        print(f"Prompt successfully dumped to {path}")
    except (IOError, OSError) as e:
        print(f"Error dumping prompt to file at {file_path}: {e}")
        raise

# Example Usage (optional - can be removed or kept for testing):
if __name__ == "__main__":
    # Define the dummy file path (inside a 'prompts' subdirectory)
    # Ensures it doesn't clutter the root if run directly
    dummy_dir = Path("prompts")
    dummy_file = dummy_dir / "dummy_prompt.md"  # Changed to .md extension

    # Create a Prompt object with the template
    example_template = "Hello, {name}! Today is {day}. Feeling {mood}?"
    prompt_to_save = Prompt(template=example_template)

    try:
        # Use dump_prompt to write the template to the file
        dump_prompt(prompt_to_save, dummy_file)

        # Get the prompt back (optional, just to show get_prompt works)
        prompt_obj = get_prompt(dummy_file)
        print(f"Loaded prompt template: '{prompt_obj.template}'")

        # Format the prompt with a missing key ('mood' is missing)
        formatted_prompt = prompt_obj.format(name="World", day="Tuesday")
        print(f"Formatted prompt (with missing key): '{formatted_prompt}'")

        # Format the prompt with all keys
        formatted_prompt_full = prompt_obj.format(name="Universe", day="Wednesday", mood="great")
        print(f"Formatted prompt (full): '{formatted_prompt_full}'")

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        pass



================================================
FILE: src/llmgine/ui/__init__.py
================================================
[Empty file]


================================================
FILE: src/llmgine/ui/cli/__init__.py
================================================
[Empty file]


================================================
FILE: src/llmgine/ui/cli/cli.py
================================================
import asyncio
import os
import sys
from dataclasses import dataclass
from typing import Any, Callable, Optional, Type

from rich.live import Live
from rich.spinner import Spinner

from llmgine.bus.bus import AsyncOrSyncCommandHandler, MessageBus
from llmgine.llm import SessionID
from llmgine.llm.engine.engine import (
    DummyEngineConfirmationInput,
    DummyEngineStatusUpdate,
    DummyEngineToolResult,
)
from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event
from llmgine.ui.cli.components import (
    CLIComponent,
    CLIPrompt,
    EngineResultComponent,
    ToolComponent,
    UserComponent,
    UserGeneralInput,
    YesNoPrompt,
)


@dataclass
class StatusEvent(Event):
    status: str = ""


class EngineCLI:
    """
    An engine CLI is a CLI that is attached to an engine.
    It needs to register the engine command using register_engine_command.
    It needs to register the engine result component using register_engine_result_component.
    It needs to register the update status event using register_update_status_event.

    It needs to attach the relevant engine events using register_engine_events.
    It needs to register the default CLI commands using register_default_cli_commands.
    It needs to validate the setup using validate_setup.
    It needs to start the main loop using main.
    """

    def __init__(self, session_id: SessionID):
        # Basic UI components
        self.components: list[UserComponent] = []
        self.component_lookup: dict[Any, Any] = {}
        self.prompt_lookup: dict[Any, Any] = {}
        self.cli_command_lookup: dict[Any, Any] = {}  # TODO what is this type?
        # Loading UI components
        self.spinner: Optional[Spinner] = None
        self.live: Optional[Live] = None
        self.hidden: bool = False
        # Bus
        self.bus: MessageBus = MessageBus()
        self.session_id: SessionID = session_id
        # Engine
        self.engine: Optional[Any] = None  # TODO rm any
        self.engine_command: Optional[Any] = None  # TODO rm any
        self.engine_result_component: Any = EngineResultComponent
        # CLI commands
        self.register_default_cli_commands()

    def register_engine(self, engine: Any):  # TODO nathan idk what this is
        self.engine = engine

    def register_engine_command(
        self, command: Type[Command], engine_function: AsyncOrSyncCommandHandler
    ):
        self.engine_command = command
        self.bus.register_command_handler(command, engine_function, self.session_id)

    def register_engine_result_component(self, component: Type[CLIComponent]):
        self.engine_result_component = component

    async def main(self):
        self.validate_setup()
        while True:
            user_input = await self.main_input()  # TODO what type is this
            if user_input is None:
                continue

            assert self.engine_command

            result = await self.bus.execute(
                self.engine_command(prompt=user_input, session_id=self.session_id)
            )
            if result.success:
                self.components.append(self.engine_result_component(result))
                self.redraw()
            else:
                print(result.error)

    def validate_setup(self):
        if self.engine is None:
            raise ValueError("Engine not attached")
        if self.engine_command is None:
            raise ValueError("Engine command not registered")
        if self.engine_result_component is None:
            raise ValueError("Engine result component not registered")

    async def main_input(self):
        prompt: UserGeneralInput = UserGeneralInput.from_prompt(
            "Do you want to continue?", self
        )
        result = await prompt.get_input()  # TODO what type is this
        self.clear_screen()
        self.redraw()
        if prompt.component is not None:
            component: UserComponent = prompt.component
            component.render()
            self.components.append(component)

        return result

    async def update_status(self, event: StatusEvent):
        if event.status == "finished":  # TODO event doesn't have status?
            await self.stop_loading()
        else:
            if not self.spinner:
                self.spinner = Spinner("point", text=f"[bold white]{event.status}")
                self.live = Live(self.spinner, refresh_per_second=10)
                self.live.start()
            else:
                if self.hidden:
                    self.live.start()
                    self.hidden = False
                self.spinner.update(text=f"[bold white]{event.status}")

    async def stop_loading(self):
        if self.live:
            self.live.stop()
            self.redraw()
        self.hidden = True

    async def component_router(self, event: Event):
        component = self.component_lookup[type(event)]
        component = component(event)
        component.render()
        self.components.append(component)

    async def prompt_router(self, command: Command) -> CommandResult:
        try:
            await self.stop_loading()
            prompt = self.prompt_lookup[type(command)]
            prompt = prompt(command)
            prompt.attach_cli(self)
            result = await prompt.get_input()
            self.clear_screen()
            self.redraw()
            if prompt.component is not None:
                component = prompt.component
                component.render()
                self.components.append(component)
            return CommandResult(success=True, result=result)
        except Exception as e:
            return CommandResult(success=False, error=str(e))

    def register_component_event(
        self, event: Type[Event], component: Type[CLIComponent]
    ) -> None:
        self.component_lookup[event] = component
        self.bus.register_event_handler(event, self.component_router, self.session_id)

    def register_prompt_command(self, command: Type[Command], prompt: CLIPrompt) -> None:
        self.prompt_lookup[command] = prompt
        self.bus.register_command_handler(command, self.prompt_router, self.session_id)

    def register_loading_event(self, event: type[Event]) -> None:
        self.bus.register_event_handler(event, self.update_status, self.session_id)

    def redraw(self) -> None:
        self.clear_screen()
        for component in self.components:
            component.render()

    def clear_screen(self) -> None:
        os.system("cls" if os.name == "nt" else "clear")

    # CLI COMMANDS STRUCTURE

    def register_cli_command(self, command: str, func: Callable) -> None:
        self.cli_command_lookup[command] = func

    def process_cli_cmds(self, input: str):
        parts = input.split(" ")
        cmd = parts[0]
        if cmd in self.cli_command_lookup:
            self.cli_command_lookup[cmd]()
            return True
        else:
            return False

    # CLI COMMANDS

    def clear_screen_cmd(self):
        self.clear_screen()

    def exit_cmd(self):
        sys.exit(0)

    def register_default_cli_commands(self):
        self.register_cli_command("clear", self.clear_screen_cmd)
        self.register_cli_command("exit", self.exit_cmd)


async def main() -> None:
    from llmgine.llm.engine.engine import DummyEngine, DummyEngineCommand

    engine = DummyEngine(SessionID("123"))
    chat = EngineCLI(SessionID("123"))
    await MessageBus().start()
    chat.register_engine(engine)
    chat.register_engine_command(DummyEngineCommand, engine.handle_command)
    chat.register_engine_result_component(EngineResultComponent)
    chat.register_loading_event(DummyEngineStatusUpdate)
    chat.register_prompt_command(DummyEngineConfirmationInput, YesNoPrompt)
    chat.register_component_event(DummyEngineToolResult, ToolComponent)
    chat.clear_screen()
    await chat.main()


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: src/llmgine/ui/cli/components.py
================================================
import asyncio
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import TYPE_CHECKING, Optional

from prompt_toolkit import HTML, PromptSession
from rich import print
from rich.panel import Panel
from rich.prompt import Confirm

from llmgine.messages.commands import Command, CommandResult
from llmgine.messages.events import Event
from llmgine.ui.cli.config import CLIConfig

if TYPE_CHECKING:
    from llmgine.ui.cli.cli import EngineCLI


class CLIComponent(ABC):
    @abstractmethod
    def render(self):
        pass

    # @abstractmethod
    # def serialize(self):
    #     pass


class CLIPrompt(ABC):
    @abstractmethod
    def get_input(self, *args, **kwargs) -> None:
        pass

    @abstractmethod
    def component(self) -> None:
        pass


@dataclass
class UserComponentEvent(Event):
    text: str = ""


class UserComponent(CLIComponent):
    """
    Event must have property text.
    """

    def __init__(self, event: Event):
        self.text = event.text

    @classmethod
    def from_text(cls, text: str):
        return cls(UserComponentEvent(text=text))

    def render(self):
        print(
            Panel(
                self.text,
                title="[bold blue]User[/bold blue]",
                subtitle_align="right",
                style="blue",
                width=CLIConfig().max_width,
                padding=CLIConfig().padding,
                title_align="left",
            )
        )

    @property
    def serialize(self):
        return {"role": "user", "content": self.text}


@dataclass
class EngineResultCommandResult(CommandResult):
    result: str = ""
    success: bool = True


class EngineResultComponent(CLIComponent):
    """
    Renders the output of the engine command. Takes in a CommandResult object.
    """

    def __init__(self, result: CommandResult):
        self.result = result.result

    def render(self):
        print(
            Panel(
                self.result,
                title="[bold green]Engine Result[/bold green]",
                style="green",
                width=CLIConfig().max_width,
                padding=CLIConfig().padding,
                title_align="left",
            )
        )


@dataclass
class AssistantResultEvent(Event):
    text: str = ""


class AssistantComponent(CLIComponent):
    """
    Event must have property text.
    """

    def __init__(self, event: Event):
        self.text = event.text

    def render(self):
        print(
            Panel(
                self.text,
                title="[bold green]Assistant[/bold green]",
                style="green",
                width=CLIConfig().max_width,
                padding=CLIConfig().padding,
                title_align="left",
            )
        )


@dataclass
class ToolResultEvent(Event):
    tool_name: str = ""
    result: str = ""


class ToolComponent(CLIComponent):
    """
    Event must have property tool_name and tool_result.
    """

    def __init__(self, event: Event):
        self.tool_name = event.tool_name
        self.tool_result = event.result

    def render(self):
        print(
            Panel(
                self.tool_result,
                title=f"[yellow][bold]:hammer_and_wrench: : {self.tool_name}[/bold][/yellow]",
                title_align="left",
                style="yellow",
                width=CLIConfig().max_width,
                padding=CLIConfig().padding,
            )
        )

    @property
    def serialize(self):
        return {"role": "tool", "content": self.tool_result}


class ToolComponentShort(CLIComponent):
    """
    Event must have property tool_name and tool_result.
    """

    def __init__(self, event: Event):
        self.tool_name = event.tool_name
        self.tool_result = event.result

    def render(self):
        print(
            Panel(
                f"[yellow][bold]:hammer_and_wrench:  Executed tool: {self.tool_name}[/bold][/yellow]",
                style="yellow",
                width=CLIConfig().max_width,
            )
        )


@dataclass
class UserGeneralInputCommand(Command):
    prompt: str = ""


class UserGeneralInput(CLIPrompt):
    """
    Command must have property prompt.
    """

    @classmethod
    def from_prompt(cls, prompt: str, cli: Optional["EngineCLI"] = None):
        return cls(UserGeneralInputCommand(prompt=prompt), cli=cli)

    def __init__(self, command: Command, cli: "EngineCLI"):
        self.session: PromptSession = PromptSession()
        self.prompt = command.prompt
        self.result = None
        self.cli = cli

    # TODO types
    async def get_input(self):
        print(
            Panel(
                "",
                title="[bold blue]User[/bold blue]",
                subtitle="[blue]Type your message... [/blue]",
                title_align="left",
                width=CLIConfig().max_width,
                style="blue",
                padding=0,
            )
        )
        while True:
            user_input = await self.session.prompt_async(
                HTML("  ‚ùØ "),
                multiline=True,
                prompt_continuation="  ‚ùØ ",
                vi_mode=CLIConfig().vi_mode,
            )
            if self.cli is not None:
                if self.cli.process_cli_cmds(user_input):
                    return None
            self.result = user_input
            return user_input

    @property
    def component(self):
        if self.result is None:
            return None
        else:
            return UserComponent.from_text(self.result)


@dataclass
class YesNoPromptCommand(Event):
    prompt: str = ""


class YesNoPrompt(CLIPrompt):
    """
    Command must have property prompt.
    """

    def __init__(self, command: EngineResultCommandResult):
        self.prompt = command.prompt
        self.result = None

    async def get_input(self):
        print(
            Panel(
                self.prompt,
                title="[bold yellow]Prompt[/bold yellow]",
                subtitle="[yellow]Type your message... (y/n)[/yellow]",
                title_align="left",
                width=CLIConfig().max_width,
                style="yellow",
                padding=CLIConfig().padding,
            )
        )
        while True:
            user_input = Confirm.ask()
            return user_input

    @property
    def component(self) -> None:
        return None

    def attach_cli(self, cli: "EngineCLI") -> None:
        self.cli = None


@dataclass
class SelectPromptCommand(Command):
    prompt: str = ""
    option_number: int = 0
    title: str = ""


class SelectPrompt(CLIPrompt):
    """
    Command must have property prompt and list.
    """

    def __init__(self, command: Command):
        self.title = command.title
        self.prompt = command.prompt

    async def get_input(self):
        print(
            Panel(
                self.prompt,
                title=f"[bold green]{self.title}[/bold green]",
                subtitle="[green]Input a number...[/green]",
                title_align="left",
                width=CLIConfig().max_width,
                style="green",
                padding=CLIConfig().padding,
            )
        )
        while True:
            user_input = IntPrompt.ask()
            return user_input

    @property
    def component(self):
        return None

    def attach_cli(self, cli: "EngineCLI"):
        self.cli = None


async def main():
    # UserComponent(UserComponentEvent(text="Hello, world!")).render()
    # AssistantComponent(AssistantResultEvent(text="Hey there!")).render()
    # ToolComponent(ToolResultEvent(tool_name="get_weather", result="Tool result")).render()
    # prompt = UserGeneralInput(
    #     UserGeneralInputCommand(prompt="Do you want to continue?"), cli=None
    # )
    # result = await prompt.get_input()
    # print(result)

    EngineResultComponent(EngineResultCommandResult(result="Hello, world!")).render()


if __name__ == "__main__":
    asyncio.run(main())



================================================
FILE: src/llmgine/ui/cli/config.py
================================================
from dataclasses import dataclass
from typing import Type, Dict, Any, Tuple, Self, cast

class Singleton:
    """
    A base class that ensures only one instance of a class exists.
    """

    _instances: Dict[Type['Singleton'], 'Singleton'] = {}

    def __new__(cls: Type[Self], *args: Any, **kwargs: Any) -> Self:
        if cls not in cls._instances:
            cls._instances[cls] = super(Singleton, cls).__new__(cls)
        return cast(Self, cls._instances[cls])


@dataclass
class CLIConfig(Singleton):
    """
    A singleton configuration class for CLI components.

    This class will always return the same instance when instantiated.
    """

    # Add your configuration fields here
    max_width: int = 9999

    padding: tuple[int, int] = (1, 2)

    vi_mode: bool = True



================================================
FILE: src/llmgine/ui/cli/voice_processing_engine_cli.py
================================================
from typing import Optional
from dataclasses import dataclass
from prompt_toolkit import PromptSession
from rich.panel import Panel
from rich import print
from prompt_toolkit import HTML, PromptSession

from llmgine.messages.commands import Command
from llmgine.ui.cli.cli import EngineCLI
from llmgine.ui.cli.components import CLIPrompt, CLIComponent
from llmgine.ui.cli.config import CLIConfig
from llmgine.messages.events import Event
@dataclass
class SpecificComponentEvent(Event):
    text: str = ""
    field: str = ""

class SpecificComponent(CLIComponent):
    """
    Event must have property text.
    """

    def __init__(self, event: SpecificComponentEvent):
        self.text : str = event.text
        self.field : str = event.field

    @classmethod
    def from_text(cls, text: str, field: str):
        return cls(SpecificComponentEvent(text=text, field=field))

    def render(self):
        print(
            Panel(
                self.text,
                title="[bold yellow]" + self.field + "[/bold yellow]",
                subtitle_align="right",
                style="yellow",
                width=CLIConfig().max_width,
                padding=CLIConfig().padding,
                title_align="left",
            )
        )

    @property
    def serialize(self):
        return {"role": "user", "content": self.field + ": " + self.text}

@dataclass
class SpecificPromptCommand(Command):
    prompt: str = ""
    field: str = ""

# Custom user prompt
class SpecificPrompt(CLIPrompt):
    """
    Command must have property prompt.
    """

    @classmethod
    def from_prompt(cls, prompt: str, cli: Optional["EngineCLI"] = None, field: str = ""):
        return cls(SpecificPromptCommand(prompt=prompt, field=field), cli=cli)

    def __init__(self, command: SpecificPromptCommand, cli: "EngineCLI"):
        self.session = PromptSession()
        self.prompt : str = command.prompt
        self.result : Optional[str] = None
        self.cli = cli
        self.field : str= command.field

    async def get_input(self):
        print(
            Panel(
                "",
                title="[bold yellow]" + self.field + "[/bold yellow]",
                subtitle="[yellow]Please enter the " + self.field + "[/yellow]",
                title_align="left",
                width=CLIConfig().max_width,
                style="yellow",
                padding=0,
            )
        )
        while True:
            user_input = await self.session.prompt_async(
                HTML("  ‚ùØ "),
                multiline=True,
                prompt_continuation="  ‚ùØ ",
                vi_mode=CLIConfig().vi_mode,
            )
            if self.cli is not None:
                if self.cli.process_cli_cmds(user_input):
                    return None
            self.result = user_input
            return user_input

    @property
    def component(self):
        if self.result is None:
            return None
        else:
            return SpecificComponent.from_text(self.result, self.field)

# ----------------------------------CUSTOM ENGINE CLI-----------------------------------

class VoiceProcessingEngineCLI(EngineCLI):
    def __init__(self, session_id: str):
        super().__init__(session_id)

    async def main(self):
        self.validate_setup()
        while True:
            audio_file = await self.main_input("Audio file")
            if audio_file is None:
                continue

            number_of_speakers = await self.main_input("Number of speakers")
            if number_of_speakers is None:
                continue

            result = await self.bus.execute(
                self.engine_command(prompt=audio_file + "&" + number_of_speakers, session_id=self.session_id)
            )
            if result.success:
                self.components.append(self.engine_result_component(result))
                self.redraw()
            else:
                print(result.error)

    async def main_input(self, field: Optional[str] = None):
        if field is None:
            prompt = SpecificPrompt.from_prompt("Do you want to continue?", self)
        else:
            prompt = SpecificPrompt.from_prompt("Do you want to continue?", self, field)

        result = await prompt.get_input()
        self.clear_screen()
        self.redraw()
        if prompt.component is not None:
            component = prompt.component
            component.render()
            self.components.append(component)
        return result



================================================
FILE: tools/__init__.py
================================================
from .test_tools import get_weather

__all__ = ["get_weather"]



================================================
FILE: tools/test_tools.py
================================================



def get_weather(city: str) -> str:
    """Get the weather for a city.
    
    Args:
        city: The city to get the weather for.

    Returns:
        The weather for the city.
    """
    return f"The weather in {city} is sunny."


